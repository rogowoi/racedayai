{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: ETL & Data Unification\n",
    "\n",
    "**Purpose:** Load all 5 triathlon data sources, normalize schemas, engineer derived features, and output unified CSVs.\n",
    "\n",
    "**Inputs:**\n",
    "- `research/data/scraped/kaggle/Half_Ironman_df6.csv` (840K rows, times in seconds)\n",
    "- `research/data/scraped/kaggle/postgres_public_tristat_stat.csv` (2.96M rows, times in HH:MM:SS)\n",
    "- `research/data/scraped/kaggle/results.csv` (1.1M rows, CoachCox, times in HH:MM:SS)\n",
    "- `research/data/scraped/kaggle/races.csv` + `series.csv` (event metadata)\n",
    "- `research/data/scraped/t100/*.csv` (~500 rows, times in seconds)\n",
    "- `research/data/scraped/wiki/*.json` (49 files, times in seconds)\n",
    "\n",
    "**Outputs:**\n",
    "- `research/data/cleaned/athlete_race.csv` (~4.8M rows)\n",
    "- `research/data/cleaned/athlete_profile.csv` (~500K unique athletes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689fcca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy>=1.24 (from -r ../requirements.txt (line 1))\n",
      "  Downloading numpy-2.4.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting pandas>=2.0 (from -r ../requirements.txt (line 2))\n",
      "  Downloading pandas-3.0.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (79 kB)\n",
      "Collecting scipy>=1.11 (from -r ../requirements.txt (line 3))\n",
      "  Downloading scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scikit-learn>=1.3 (from -r ../requirements.txt (line 4))\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mykyta/projects/indie/racedayai/.conda/lib/python3.11/site-packages (from pandas>=2.0->-r ../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn>=1.3->-r ../requirements.txt (line 4))\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn>=1.3->-r ../requirements.txt (line 4))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mykyta/projects/indie/racedayai/.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0->-r ../requirements.txt (line 2)) (1.17.0)\n",
      "Downloading numpy-2.4.2-cp311-cp311-macosx_14_0_arm64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-3.0.0-cp311-cp311-macosx_11_0_arm64.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.17.0-cp311-cp311-macosx_14_0_arm64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.8.0-cp311-cp311-macosx_12_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 numpy-2.4.2 pandas-3.0.0 scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /Users/mykyta/projects/indie/racedayai/research\n",
      "Scraped data:  /Users/mykyta/projects/indie/racedayai/research/data/scraped\n",
      "Cleaned output: /Users/mykyta/projects/indie/racedayai/research/data/cleaned\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path('..').resolve()\n",
    "SCRAPED_DIR = BASE_DIR / 'data' / 'scraped'\n",
    "CLEANED_DIR = BASE_DIR / 'data' / 'cleaned'\n",
    "CLEANED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Scraped data:  {SCRAPED_DIR}\")\n",
    "print(f\"Cleaned output: {CLEANED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# ─── Time Conversion ───────────────────────────────────────────────\n",
    "def hh_mm_ss_to_seconds(time_str):\n",
    "    \"\"\"Convert HH:MM:SS or H:MM:SS or M:SS string to seconds.\n",
    "    Returns NaN for missing/invalid values.\"\"\"\n",
    "    if pd.isna(time_str) or str(time_str).strip() in ('', '0:00', '0:00:00', '--', 'DNS', 'DNF', 'DQ'):\n",
    "        return np.nan\n",
    "    try:\n",
    "        time_str = str(time_str).strip()\n",
    "        parts = time_str.split(':')\n",
    "        if len(parts) == 3:\n",
    "            h, m, s = parts\n",
    "            return int(h) * 3600 + int(m) * 60 + float(s)\n",
    "        elif len(parts) == 2:\n",
    "            m, s = parts\n",
    "            total = int(m) * 60 + float(s)\n",
    "            # If > 3600 seconds from MM:SS, it's probably HH:MM misinterpreted\n",
    "            return total\n",
    "        else:\n",
    "            return float(time_str)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def normalize_gender(val):\n",
    "    \"\"\"Normalize gender to M/F.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    val = str(val).strip().upper()\n",
    "    if val in ('M', 'MALE'):\n",
    "        return 'M'\n",
    "    elif val in ('F', 'FEMALE'):\n",
    "        return 'F'\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def extract_age_group(text, default=None):\n",
    "    \"\"\"Extract age group like '40-44' from various formats.\n",
    "    Handles: '40-44', 'M40-44', 'F45-49', 'MAGE40-44', etc.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return default\n",
    "    text = str(text).strip()\n",
    "    match = re.search(r'(\\d{2,3})-(\\d{2,3})', text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-{match.group(2)}\"\n",
    "    # Handle single age like \"70+\" or \"75+\"\n",
    "    match = re.search(r'(\\d{2,3})\\+', text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}+\"\n",
    "    return default\n",
    "\n",
    "\n",
    "def extract_age_band(age_group):\n",
    "    \"\"\"Extract lower bound of age group as integer. E.g., '40-44' -> 40.\"\"\"\n",
    "    if pd.isna(age_group):\n",
    "        return np.nan\n",
    "    match = re.search(r'(\\d{2,3})', str(age_group))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def hash_athlete(name, country, gender):\n",
    "    \"\"\"Create a deterministic hash for athlete deduplication.\"\"\"\n",
    "    if pd.isna(name) or str(name).strip() == '':\n",
    "        return None\n",
    "    key = f\"{str(name).strip().lower()}|{str(country).strip().lower()}|{str(gender).strip().upper()}\"\n",
    "    return hashlib.sha256(key.encode()).hexdigest()[:16]\n",
    "\n",
    "\n",
    "print(\"Utility functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality rules loaded.\n"
     ]
    }
   ],
   "source": [
    "# ─── Data Quality Rules (from Plan 07, Appendix A) ─────────────────\n",
    "QUALITY_RULES = {\n",
    "    'min_swim_sec': 600,       # 10 min minimum swim\n",
    "    'max_swim_sec': 7200,      # 2 hours max swim\n",
    "    'min_bike_sec': 1800,      # 30 min minimum bike\n",
    "    'max_bike_sec': 36000,     # 10 hours max bike\n",
    "    'min_run_sec': 900,        # 15 min minimum run\n",
    "    'max_run_sec': 25200,      # 7 hours max run\n",
    "    'min_total_sec': 3600,     # 1 hour minimum total\n",
    "    'max_total_sec': 61200,    # 17 hours maximum total\n",
    "    'max_transition_sec': 600, # 10 min max per transition\n",
    "    'sum_tolerance': 120,      # total must equal sum of splits ± 120 sec\n",
    "    'min_split_pct': 0.02,     # no segment < 2% of total\n",
    "    'max_split_pct': 0.70,     # no segment > 70% of total\n",
    "}\n",
    "\n",
    "\n",
    "def apply_quality_filters(df, label=\"\"):\n",
    "    \"\"\"Apply quality rules and return cleaned DataFrame + anomaly count.\"\"\"\n",
    "    n_before = len(df)\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    reasons = []\n",
    "\n",
    "    # Total time bounds\n",
    "    if 'total_sec' in df.columns:\n",
    "        bad = (df['total_sec'] < QUALITY_RULES['min_total_sec']) | (df['total_sec'] > QUALITY_RULES['max_total_sec'])\n",
    "        mask &= ~bad\n",
    "        reasons.append(f\"total_sec bounds: {bad.sum()}\")\n",
    "\n",
    "    # Segment bounds (only check if column exists and is not all NaN)\n",
    "    for seg, col in [('swim', 'swim_sec'), ('bike', 'bike_sec'), ('run', 'run_sec')]:\n",
    "        if col in df.columns and df[col].notna().any():\n",
    "            bad = (df[col].notna()) & ((df[col] < QUALITY_RULES[f'min_{seg}_sec']) | (df[col] > QUALITY_RULES[f'max_{seg}_sec']))\n",
    "            mask &= ~bad\n",
    "            reasons.append(f\"{col} bounds: {bad.sum()}\")\n",
    "\n",
    "    # Transition bounds\n",
    "    for col in ['t1_sec', 't2_sec']:\n",
    "        if col in df.columns and df[col].notna().any():\n",
    "            bad = (df[col].notna()) & (df[col] > QUALITY_RULES['max_transition_sec'])\n",
    "            mask &= ~bad\n",
    "            reasons.append(f\"{col} bounds: {bad.sum()}\")\n",
    "\n",
    "    # Sum tolerance: check that splits sum to total\n",
    "    split_cols = [c for c in ['swim_sec', 't1_sec', 'bike_sec', 't2_sec', 'run_sec'] if c in df.columns]\n",
    "    if len(split_cols) >= 3 and 'total_sec' in df.columns:\n",
    "        split_sum = df[split_cols].sum(axis=1)\n",
    "        # Only check where all splits are non-null\n",
    "        all_present = df[split_cols].notna().all(axis=1)\n",
    "        bad = all_present & ((split_sum - df['total_sec']).abs() > QUALITY_RULES['sum_tolerance'])\n",
    "        mask &= ~bad\n",
    "        reasons.append(f\"sum tolerance: {bad.sum()}\")\n",
    "\n",
    "    # Split percentage bounds (no segment > 70% or < 2%)\n",
    "    if 'total_sec' in df.columns:\n",
    "        for col in ['swim_sec', 'bike_sec', 'run_sec']:\n",
    "            if col in df.columns and df[col].notna().any():\n",
    "                pct = df[col] / df['total_sec']\n",
    "                bad = (pct.notna()) & ((pct < QUALITY_RULES['min_split_pct']) | (pct > QUALITY_RULES['max_split_pct']))\n",
    "                mask &= ~bad\n",
    "                reasons.append(f\"{col} pct bounds: {bad.sum()}\")\n",
    "\n",
    "    # Negative or zero total\n",
    "    if 'total_sec' in df.columns:\n",
    "        bad = df['total_sec'] <= 0\n",
    "        mask &= ~bad\n",
    "        reasons.append(f\"non-positive total: {bad.sum()}\")\n",
    "\n",
    "    df_clean = df[mask].copy()\n",
    "    n_after = len(df_clean)\n",
    "    print(f\"  [{label}] Quality filter: {n_before:,} → {n_after:,} ({n_before - n_after:,} removed)\")\n",
    "    for r in reasons:\n",
    "        if not r.endswith(': 0'):\n",
    "            print(f\"    - {r}\")\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "print(\"Quality rules loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load df6 (Kaggle Half-Ironman) — 840K rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df6 loaded: 840,075 rows, 13 columns\n",
      "Columns: ['Gender', 'AgeGroup', 'AgeBand', 'Country', 'CountryISO2', 'EventYear', 'EventLocation', 'SwimTime', 'Transition1Time', 'BikeTime', 'Transition2Time', 'RunTime', 'FinishTime']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>AgeGroup</th>\n",
       "      <th>AgeBand</th>\n",
       "      <th>Country</th>\n",
       "      <th>CountryISO2</th>\n",
       "      <th>EventYear</th>\n",
       "      <th>EventLocation</th>\n",
       "      <th>SwimTime</th>\n",
       "      <th>Transition1Time</th>\n",
       "      <th>BikeTime</th>\n",
       "      <th>Transition2Time</th>\n",
       "      <th>RunTime</th>\n",
       "      <th>FinishTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>40-44</td>\n",
       "      <td>40</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>2019</td>\n",
       "      <td>IRONMAN 70.3 South American Championship Bueno...</td>\n",
       "      <td>1679</td>\n",
       "      <td>119</td>\n",
       "      <td>9107</td>\n",
       "      <td>95</td>\n",
       "      <td>5515</td>\n",
       "      <td>16514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>45-49</td>\n",
       "      <td>45</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>2019</td>\n",
       "      <td>IRONMAN 70.3 South American Championship Bueno...</td>\n",
       "      <td>2070</td>\n",
       "      <td>177</td>\n",
       "      <td>9160</td>\n",
       "      <td>132</td>\n",
       "      <td>6070</td>\n",
       "      <td>17609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>45-49</td>\n",
       "      <td>45</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>AD</td>\n",
       "      <td>2020</td>\n",
       "      <td>IRONMAN 70.3 Bariloche</td>\n",
       "      <td>1667</td>\n",
       "      <td>161</td>\n",
       "      <td>9891</td>\n",
       "      <td>122</td>\n",
       "      <td>5190</td>\n",
       "      <td>17031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender AgeGroup  AgeBand  Country CountryISO2  EventYear                                      EventLocation  SwimTime  Transition1Time  BikeTime  Transition2Time  RunTime  FinishTime\n",
       "0      M    40-44       40  Andorra          AD       2019  IRONMAN 70.3 South American Championship Bueno...      1679              119      9107               95     5515       16514\n",
       "1      M    45-49       45  Andorra          AD       2019  IRONMAN 70.3 South American Championship Bueno...      2070              177      9160              132     6070       17609\n",
       "2      M    45-49       45  Andorra          AD       2020                             IRONMAN 70.3 Bariloche      1667              161      9891              122     5190       17031"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load df6 — times are already in seconds\n",
    "df6_path = SCRAPED_DIR / 'kaggle' / 'Half_Ironman_df6.csv'\n",
    "df6_raw = pd.read_csv(df6_path)\n",
    "print(f\"df6 loaded: {len(df6_raw):,} rows, {len(df6_raw.columns)} columns\")\n",
    "print(f\"Columns: {list(df6_raw.columns)}\")\n",
    "df6_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [df6] Quality filter: 840,075 → 840,073 (2 removed)\n",
      "    - sum tolerance: 1\n",
      "    - bike_sec pct bounds: 2\n",
      "\n",
      "df6 final: 840,073 rows\n",
      "Gender distribution:\n",
      "gender\n",
      "M    635678\n",
      "F    204395\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distance: <StringArray>\n",
      "['70.3']\n",
      "Length: 1, dtype: str\n",
      "Year range: 2004 - 2020\n"
     ]
    }
   ],
   "source": [
    "# Normalize df6 into unified schema\n",
    "df6 = pd.DataFrame({\n",
    "    'athlete_hash': None,\n",
    "    'athlete_name': None,\n",
    "    'gender': df6_raw['Gender'],\n",
    "    'age_group': df6_raw['AgeGroup'],\n",
    "    'age_band': df6_raw['AgeBand'].astype(float),\n",
    "    'country': df6_raw['Country'],\n",
    "    'country_iso2': df6_raw['CountryISO2'],\n",
    "    'is_pro': False,\n",
    "    'event_name': df6_raw['EventLocation'],  # EventLocation contains the full event name\n",
    "    'event_year': df6_raw['EventYear'].astype(int),\n",
    "    'event_location': df6_raw['EventLocation'].str.extract(r'IRONMAN 70\\.3 (.+)')[0],\n",
    "    'event_distance': '70.3',\n",
    "    'source': 'kaggle_df6',\n",
    "    'swim_sec': df6_raw['SwimTime'].astype(float),\n",
    "    't1_sec': df6_raw['Transition1Time'].astype(float),\n",
    "    'bike_sec': df6_raw['BikeTime'].astype(float),\n",
    "    't2_sec': df6_raw['Transition2Time'].astype(float),\n",
    "    'run_sec': df6_raw['RunTime'].astype(float),\n",
    "    'total_sec': df6_raw['FinishTime'].astype(float),\n",
    "    'overall_rank': np.nan,\n",
    "    'gender_rank': np.nan,\n",
    "    'age_group_rank': np.nan,\n",
    "    'division_rank': np.nan,\n",
    "    'finish_status': 'finisher',\n",
    "})\n",
    "\n",
    "# Apply quality filters\n",
    "df6 = apply_quality_filters(df6, label='df6')\n",
    "print(f\"\\ndf6 final: {len(df6):,} rows\")\n",
    "print(f\"Gender distribution:\\n{df6['gender'].value_counts()}\")\n",
    "print(f\"\\nDistance: {df6['event_distance'].unique()}\")\n",
    "print(f\"Year range: {df6['event_year'].min()} - {df6['event_year'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load TriStat — 2.96M rows (HH:MM:SS format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TriStat (this may take a moment)...\n",
      "TriStat loaded: 2,961,502 rows, 12 columns\n",
      "Columns: ['event_link', 'gender', 'person_link', 'person_flag', 'person_name', 'person_event_group', 'person_event_swim_time_text', 'person_event_t1_time_text', 'person_event_cycle_time_text', 'person_event_t2_time_text', 'person_event_run_time_text', 'person_event_finish_time_text']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_link</th>\n",
       "      <th>gender</th>\n",
       "      <th>person_link</th>\n",
       "      <th>person_flag</th>\n",
       "      <th>person_name</th>\n",
       "      <th>person_event_group</th>\n",
       "      <th>person_event_swim_time_text</th>\n",
       "      <th>person_event_t1_time_text</th>\n",
       "      <th>person_event_cycle_time_text</th>\n",
       "      <th>person_event_t2_time_text</th>\n",
       "      <th>person_event_run_time_text</th>\n",
       "      <th>person_event_finish_time_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>/irl/profile/halliwell-mark</td>\n",
       "      <td>IRL</td>\n",
       "      <td>Halliwell, Mark</td>\n",
       "      <td>M45-49</td>\n",
       "      <td>0:00</td>\n",
       "      <td>0:00</td>\n",
       "      <td>7:52:05</td>\n",
       "      <td>19:00</td>\n",
       "      <td>5:26:34</td>\n",
       "      <td>13:37:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>F</td>\n",
       "      <td>/usa/profile/harris-polly</td>\n",
       "      <td>USA</td>\n",
       "      <td>Harris, Polly</td>\n",
       "      <td>F50-54</td>\n",
       "      <td>0:00</td>\n",
       "      <td>0:00</td>\n",
       "      <td>7:58:53</td>\n",
       "      <td>16:37</td>\n",
       "      <td>5:22:09</td>\n",
       "      <td>13:37:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/rus/result/ironman/ireland-cork/full/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>/fra/profile/peugeot-rodolphe</td>\n",
       "      <td>FRA</td>\n",
       "      <td>Peugeot, Rodolphe</td>\n",
       "      <td>M25-29</td>\n",
       "      <td>0:00</td>\n",
       "      <td>0:00</td>\n",
       "      <td>7:57:05</td>\n",
       "      <td>10:40</td>\n",
       "      <td>5:30:10</td>\n",
       "      <td>13:37:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   event_link gender                    person_link person_flag        person_name person_event_group person_event_swim_time_text person_event_t1_time_text  \\\n",
       "0  /rus/result/ironman/ireland-cork/full/2019      M    /irl/profile/halliwell-mark         IRL    Halliwell, Mark             M45-49                        0:00                      0:00   \n",
       "1  /rus/result/ironman/ireland-cork/full/2019      F      /usa/profile/harris-polly         USA      Harris, Polly             F50-54                        0:00                      0:00   \n",
       "2  /rus/result/ironman/ireland-cork/full/2019      M  /fra/profile/peugeot-rodolphe         FRA  Peugeot, Rodolphe             M25-29                        0:00                      0:00   \n",
       "\n",
       "  person_event_cycle_time_text person_event_t2_time_text person_event_run_time_text person_event_finish_time_text  \n",
       "0                      7:52:05                     19:00                    5:26:34                      13:37:39  \n",
       "1                      7:58:53                     16:37                    5:22:09                      13:37:40  \n",
       "2                      7:57:05                     10:40                    5:30:10                      13:37:55  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load TriStat in chunks to manage memory\n",
    "tristat_path = SCRAPED_DIR / 'kaggle' / 'postgres_public_tristat_stat.csv'\n",
    "\n",
    "# Read with low_memory=False for type inference\n",
    "print(\"Loading TriStat (this may take a moment)...\")\n",
    "tristat_raw = pd.read_csv(tristat_path, low_memory=False)\n",
    "print(f\"TriStat loaded: {len(tristat_raw):,} rows, {len(tristat_raw.columns)} columns\")\n",
    "print(f\"Columns: {list(tristat_raw.columns)}\")\n",
    "tristat_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting TriStat times from HH:MM:SS to seconds...\n",
      "  person_event_swim_time_text → swim_sec: 2,848,108 valid values (96.2%)\n",
      "  person_event_t1_time_text → t1_sec: 2,618,460 valid values (88.4%)\n",
      "  person_event_cycle_time_text → bike_sec: 2,895,272 valid values (97.8%)\n",
      "  person_event_t2_time_text → t2_sec: 2,756,030 valid values (93.1%)\n",
      "  person_event_run_time_text → run_sec: 2,907,335 valid values (98.2%)\n",
      "  person_event_finish_time_text → total_sec: 2,961,502 valid values (100.0%)\n",
      "\n",
      "Records with valid total_sec: 2,961,502\n",
      "Records with all NaN times: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert TriStat HH:MM:SS times to seconds\n",
    "print(\"Converting TriStat times from HH:MM:SS to seconds...\")\n",
    "time_cols_map = {\n",
    "    'person_event_swim_time_text': 'swim_sec',\n",
    "    'person_event_t1_time_text': 't1_sec',\n",
    "    'person_event_cycle_time_text': 'bike_sec',\n",
    "    'person_event_t2_time_text': 't2_sec',\n",
    "    'person_event_run_time_text': 'run_sec',\n",
    "    'person_event_finish_time_text': 'total_sec',\n",
    "}\n",
    "\n",
    "for src_col, dst_col in time_cols_map.items():\n",
    "    tristat_raw[dst_col] = tristat_raw[src_col].apply(hh_mm_ss_to_seconds)\n",
    "    n_valid = tristat_raw[dst_col].notna().sum()\n",
    "    print(f\"  {src_col} → {dst_col}: {n_valid:,} valid values ({n_valid/len(tristat_raw)*100:.1f}%)\")\n",
    "\n",
    "# Check how many have valid total_sec\n",
    "print(f\"\\nRecords with valid total_sec: {tristat_raw['total_sec'].notna().sum():,}\")\n",
    "print(f\"Records with all NaN times: {tristat_raw[list(time_cols_map.values())].isna().all(axis=1).sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance distribution:\n",
      "event_distance\n",
      "70.3       1492594\n",
      "140.6      1020358\n",
      "olympic     277907\n",
      "sprint      170643\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Year range: 1983.0 - 2022.0\n"
     ]
    }
   ],
   "source": [
    "# Parse TriStat metadata\n",
    "# Extract event info from event_link (e.g., '/rus/result/ironman/ireland-cork/full/2019')\n",
    "def parse_tristat_event_link(link):\n",
    "    \"\"\"Parse event_link to extract distance and event info.\"\"\"\n",
    "    if pd.isna(link):\n",
    "        return '140.6', None, None, None  # default to full distance\n",
    "    link = str(link).lower()\n",
    "\n",
    "    # Distance detection\n",
    "    if '70.3' in link or 'half' in link:\n",
    "        distance = '70.3'\n",
    "    elif 'sprint' in link:\n",
    "        distance = 'sprint'\n",
    "    elif 'olympic' in link:\n",
    "        distance = 'olympic'\n",
    "    else:\n",
    "        distance = '140.6'  # default full\n",
    "\n",
    "    # Extract location (between /ironman/ and /full|half/)\n",
    "    parts = link.strip('/').split('/')\n",
    "    location = None\n",
    "    year = None\n",
    "    for i, p in enumerate(parts):\n",
    "        if p == 'ironman' and i + 1 < len(parts):\n",
    "            location = parts[i + 1].replace('-', ' ').title()\n",
    "        # Year is usually the last numeric part\n",
    "        if re.match(r'^\\d{4}$', p):\n",
    "            year = int(p)\n",
    "\n",
    "    event_name = f\"Ironman {distance} {location}\" if location else None\n",
    "    return distance, event_name, location, year\n",
    "\n",
    "# Apply parsing\n",
    "parsed = tristat_raw['event_link'].apply(parse_tristat_event_link)\n",
    "tristat_raw['event_distance'] = parsed.apply(lambda x: x[0])\n",
    "tristat_raw['event_name_parsed'] = parsed.apply(lambda x: x[1])\n",
    "tristat_raw['event_location_parsed'] = parsed.apply(lambda x: x[2])\n",
    "tristat_raw['event_year_parsed'] = parsed.apply(lambda x: x[3])\n",
    "\n",
    "print(f\"Distance distribution:\\n{tristat_raw['event_distance'].value_counts()}\")\n",
    "print(f\"\\nYear range: {tristat_raw['event_year_parsed'].min()} - {tristat_raw['event_year_parsed'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TriStat after dropping NaN total: 2,961,502 → 2,961,502\n",
      "  [tristat] Quality filter: 2,961,502 → 2,426,245 (535,257 removed)\n",
      "    - total_sec bounds: 38953\n",
      "    - swim_sec bounds: 61421\n",
      "    - bike_sec bounds: 34878\n",
      "    - run_sec bounds: 42399\n",
      "    - t1_sec bounds: 342247\n",
      "    - t2_sec bounds: 220598\n",
      "    - sum tolerance: 22772\n",
      "    - swim_sec pct bounds: 8608\n",
      "    - bike_sec pct bounds: 5373\n",
      "    - run_sec pct bounds: 17656\n",
      "\n",
      "TriStat final: 2,426,245 rows\n",
      "Gender: {'M': 1855771, 'F': 570469}\n",
      "Distances: {'70.3': 1363349, '140.6': 690598, 'olympic': 259692, 'sprint': 112606}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build normalized TriStat DataFrame\n",
    "tristat = pd.DataFrame({\n",
    "    'athlete_hash': tristat_raw.apply(\n",
    "        lambda r: hash_athlete(r['person_name'], r.get('person_flag', ''), r['gender']), axis=1\n",
    "    ),\n",
    "    'athlete_name': tristat_raw['person_name'],\n",
    "    'gender': tristat_raw['gender'].apply(normalize_gender),\n",
    "    'age_group': tristat_raw['person_event_group'].apply(extract_age_group),\n",
    "    'age_band': tristat_raw['person_event_group'].apply(extract_age_group).apply(extract_age_band),\n",
    "    'country': tristat_raw['person_flag'],\n",
    "    'country_iso2': tristat_raw['person_flag'],  # person_flag is ISO code\n",
    "    'is_pro': False,  # TriStat is mostly AG\n",
    "    'event_name': tristat_raw['event_name_parsed'],\n",
    "    'event_year': tristat_raw['event_year_parsed'],\n",
    "    'event_location': tristat_raw['event_location_parsed'],\n",
    "    'event_distance': tristat_raw['event_distance'],\n",
    "    'source': 'tristat',\n",
    "    'swim_sec': tristat_raw['swim_sec'],\n",
    "    't1_sec': tristat_raw['t1_sec'],\n",
    "    'bike_sec': tristat_raw['bike_sec'],\n",
    "    't2_sec': tristat_raw['t2_sec'],\n",
    "    'run_sec': tristat_raw['run_sec'],\n",
    "    'total_sec': tristat_raw['total_sec'],\n",
    "    'overall_rank': np.nan,\n",
    "    'gender_rank': np.nan,\n",
    "    'age_group_rank': np.nan,\n",
    "    'division_rank': np.nan,\n",
    "    'finish_status': 'finisher',\n",
    "})\n",
    "\n",
    "# Drop rows where total_sec is NaN (no valid time data at all)\n",
    "n_before = len(tristat)\n",
    "tristat = tristat.dropna(subset=['total_sec'])\n",
    "print(f\"TriStat after dropping NaN total: {n_before:,} → {len(tristat):,}\")\n",
    "\n",
    "# Apply quality filters\n",
    "tristat = apply_quality_filters(tristat, label='tristat')\n",
    "print(f\"\\nTriStat final: {len(tristat):,} rows\")\n",
    "print(f\"Gender: {tristat['gender'].value_counts().to_dict()}\")\n",
    "print(f\"Distances: {tristat['event_distance'].value_counts().to_dict()}\")\n",
    "\n",
    "# Free raw memory\n",
    "del tristat_raw\n",
    "import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load CoachCox — 1.1M rows (HH:MM:SS format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoachCox results: 1,096,719 rows\n",
      "CoachCox races: 587 rows\n",
      "CoachCox series: 69 rows\n",
      "\n",
      "Results columns: ['bib', 'Name', 'athleteLink', 'Country', 'Gender', 'Division', 'divLink', 'divisionRank', 'overallTime', 'overallRank', 'swimTime', 'swimRank', 'bikeTime', 'bikeRank', 'runTime', 'runRank', 'finishStatus', 'raceID', 'athleteID']\n",
      "\n",
      "Finish status distribution:\n",
      "finishStatus\n",
      "Finisher    892707\n",
      "DNF         105522\n",
      "DNS          95203\n",
      "DQ            3169\n",
      "NC             118\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bib</th>\n",
       "      <th>Name</th>\n",
       "      <th>athleteLink</th>\n",
       "      <th>Country</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Division</th>\n",
       "      <th>divLink</th>\n",
       "      <th>divisionRank</th>\n",
       "      <th>overallTime</th>\n",
       "      <th>overallRank</th>\n",
       "      <th>swimTime</th>\n",
       "      <th>swimRank</th>\n",
       "      <th>bikeTime</th>\n",
       "      <th>bikeRank</th>\n",
       "      <th>runTime</th>\n",
       "      <th>runRank</th>\n",
       "      <th>finishStatus</th>\n",
       "      <th>raceID</th>\n",
       "      <th>athleteID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>Nils Frommhold</td>\n",
       "      <td>https://www.coachcox.co.uk/imstats/athlete/28963/</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>MPRO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8:03:13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4:22:45</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2:48:06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Finisher</td>\n",
       "      <td>1</td>\n",
       "      <td>28963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>Paul Matthews</td>\n",
       "      <td>https://www.coachcox.co.uk/imstats/athlete/143...</td>\n",
       "      <td>United States</td>\n",
       "      <td>Male</td>\n",
       "      <td>MPRO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8:04:58</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48:27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4:24:31</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2:48:27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Finisher</td>\n",
       "      <td>1</td>\n",
       "      <td>143770.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tj Tollakson</td>\n",
       "      <td>https://www.coachcox.co.uk/imstats/athlete/33054/</td>\n",
       "      <td>United States</td>\n",
       "      <td>Male</td>\n",
       "      <td>MPRO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8:07:36</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48:34</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4:19:03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2:56:01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Finisher</td>\n",
       "      <td>1</td>\n",
       "      <td>33054.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bib            Name                                        athleteLink        Country Gender Division divLink  divisionRank overallTime  overallRank swimTime  swimRank bikeTime  bikeRank  \\\n",
       "0   40  Nils Frommhold  https://www.coachcox.co.uk/imstats/athlete/28963/        Germany   Male     MPRO     NaN           1.0     8:03:13          1.0    48:19       1.0  4:22:45       3.0   \n",
       "1   58   Paul Matthews  https://www.coachcox.co.uk/imstats/athlete/143...  United States   Male     MPRO     NaN           2.0     8:04:58          2.0    48:27       4.0  4:24:31       7.0   \n",
       "2    3    Tj Tollakson  https://www.coachcox.co.uk/imstats/athlete/33054/  United States   Male     MPRO     NaN           3.0     8:07:36          3.0    48:34       9.0  4:19:03       2.0   \n",
       "\n",
       "   runTime  runRank finishStatus  raceID  athleteID  \n",
       "0  2:48:06      1.0     Finisher       1    28963.0  \n",
       "1  2:48:27      2.0     Finisher       1   143770.0  \n",
       "2  2:56:01      8.0     Finisher       1    33054.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CoachCox results + races + series\n",
    "cc_results_path = SCRAPED_DIR / 'kaggle' / 'results.csv'\n",
    "cc_races_path = SCRAPED_DIR / 'kaggle' / 'races.csv'\n",
    "cc_series_path = SCRAPED_DIR / 'kaggle' / 'series.csv'\n",
    "\n",
    "cc_raw = pd.read_csv(cc_results_path, low_memory=False)\n",
    "cc_races = pd.read_csv(cc_races_path)\n",
    "cc_series = pd.read_csv(cc_series_path)\n",
    "\n",
    "print(f\"CoachCox results: {len(cc_raw):,} rows\")\n",
    "print(f\"CoachCox races: {len(cc_races):,} rows\")\n",
    "print(f\"CoachCox series: {len(cc_series):,} rows\")\n",
    "print(f\"\\nResults columns: {list(cc_raw.columns)}\")\n",
    "print(f\"\\nFinish status distribution:\\n{cc_raw['finishStatus'].value_counts()}\")\n",
    "cc_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched races: 587\n",
      "Sample location: ['Ironman Alaska', 'Ironman Arizona', 'Ironman Arizona']\n",
      "Results after join: 1,096,719\n"
     ]
    }
   ],
   "source": [
    "# Join races and series for event metadata\n",
    "cc_races_enriched = cc_races.merge(cc_series, left_on='seriesID', right_on='id', suffixes=('_race', '_series'))\n",
    "print(f\"Enriched races: {len(cc_races_enriched):,}\")\n",
    "print(f\"Sample location: {cc_races_enriched['location'].head(3).tolist()}\")\n",
    "\n",
    "# Join results with race metadata\n",
    "cc_raw = cc_raw.merge(\n",
    "    cc_races_enriched[['id_race', 'year', 'location', 'continent', 'dnf', 'finishers']],\n",
    "    left_on='raceID', right_on='id_race', how='left'\n",
    ")\n",
    "print(f\"Results after join: {len(cc_raw):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CoachCox times from HH:MM:SS to seconds...\n",
      "  swimTime → swim_sec: 947,570 valid (86.4%)\n",
      "  bikeTime → bike_sec: 935,797 valid (85.3%)\n",
      "  runTime → run_sec: 889,711 valid (81.1%)\n",
      "  overallTime → total_sec: 890,912 valid (81.2%)\n",
      "\n",
      "Pro athletes: 20,727 (1.9%)\n",
      "Division examples: {'M40-44': 176419, 'M35-39': 154891, 'M45-49': 146198, 'M30-34': 121919, 'M50-54': 101959, 'M25-29': 66923, 'M55-59': 49929, 'F40-44': 38088, 'F35-39': 34576, 'F30-34': 31687}\n"
     ]
    }
   ],
   "source": [
    "# Convert CoachCox times\n",
    "print(\"Converting CoachCox times from HH:MM:SS to seconds...\")\n",
    "for src_col, dst_col in [('swimTime', 'swim_sec'), ('bikeTime', 'bike_sec'),\n",
    "                          ('runTime', 'run_sec'), ('overallTime', 'total_sec')]:\n",
    "    cc_raw[dst_col] = cc_raw[src_col].apply(hh_mm_ss_to_seconds)\n",
    "    n_valid = cc_raw[dst_col].notna().sum()\n",
    "    print(f\"  {src_col} → {dst_col}: {n_valid:,} valid ({n_valid/len(cc_raw)*100:.1f}%)\")\n",
    "\n",
    "# CoachCox doesn't have T1/T2 separately\n",
    "cc_raw['t1_sec'] = np.nan\n",
    "cc_raw['t2_sec'] = np.nan\n",
    "\n",
    "# Detect pro athletes\n",
    "cc_raw['is_pro'] = cc_raw['Division'].str.upper().isin(['MPRO', 'FPRO'])\n",
    "print(f\"\\nPro athletes: {cc_raw['is_pro'].sum():,} ({cc_raw['is_pro'].mean()*100:.1f}%)\")\n",
    "print(f\"Division examples: {cc_raw['Division'].value_counts().head(10).to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoachCox after dropping NaN total: 1,096,719 → 890,912\n",
      "  [coachcox_finishers] Quality filter: 890,154 → 864,224 (25,930 removed)\n",
      "    - total_sec bounds: 350\n",
      "    - swim_sec bounds: 8655\n",
      "    - bike_sec bounds: 19\n",
      "    - run_sec bounds: 16225\n",
      "    - swim_sec pct bounds: 1489\n",
      "    - bike_sec pct bounds: 32\n",
      "    - run_sec pct bounds: 1951\n",
      "\n",
      "CoachCox final: 864,982 rows\n",
      "  Finishers: 864,224, Non-finishers: 758\n",
      "  Gender: {'M': 705117, 'F': 159683}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build normalized CoachCox DataFrame\n",
    "coachcox = pd.DataFrame({\n",
    "    'athlete_hash': cc_raw.apply(\n",
    "        lambda r: hash_athlete(r['Name'], r['Country'], r['Gender']), axis=1\n",
    "    ),\n",
    "    'athlete_name': cc_raw['Name'],\n",
    "    'gender': cc_raw['Gender'].apply(normalize_gender),\n",
    "    'age_group': cc_raw['Division'].apply(extract_age_group),\n",
    "    'age_band': cc_raw['Division'].apply(extract_age_group).apply(extract_age_band),\n",
    "    'country': cc_raw['Country'],\n",
    "    'country_iso2': np.nan,  # Not available directly\n",
    "    'is_pro': cc_raw['is_pro'],\n",
    "    'event_name': cc_raw['location'],\n",
    "    'event_year': cc_raw['year'],\n",
    "    'event_location': cc_raw['location'],\n",
    "    'event_distance': '140.6',  # CoachCox is full Ironman\n",
    "    'source': 'coachcox',\n",
    "    'swim_sec': cc_raw['swim_sec'],\n",
    "    't1_sec': cc_raw['t1_sec'],\n",
    "    'bike_sec': cc_raw['bike_sec'],\n",
    "    't2_sec': cc_raw['t2_sec'],\n",
    "    'run_sec': cc_raw['run_sec'],\n",
    "    'total_sec': cc_raw['total_sec'],\n",
    "    'overall_rank': pd.to_numeric(cc_raw['overallRank'], errors='coerce'),\n",
    "    'gender_rank': np.nan,\n",
    "    'age_group_rank': np.nan,\n",
    "    'division_rank': pd.to_numeric(cc_raw['divisionRank'], errors='coerce'),\n",
    "    'finish_status': cc_raw['finishStatus'].str.lower().str.strip(),\n",
    "})\n",
    "\n",
    "# Drop rows without valid total time\n",
    "n_before = len(coachcox)\n",
    "coachcox = coachcox.dropna(subset=['total_sec'])\n",
    "print(f\"CoachCox after dropping NaN total: {n_before:,} → {len(coachcox):,}\")\n",
    "\n",
    "# Apply quality filters (only to finishers; DNF/DNS won't have valid times)\n",
    "cc_finishers = coachcox[coachcox['finish_status'] == 'finisher'].copy()\n",
    "cc_nonfinishers = coachcox[coachcox['finish_status'] != 'finisher'].copy()\n",
    "\n",
    "cc_finishers = apply_quality_filters(cc_finishers, label='coachcox_finishers')\n",
    "\n",
    "# Keep non-finishers (DNF/DNS/DQ) without quality filtering their times\n",
    "coachcox = pd.concat([cc_finishers, cc_nonfinishers], ignore_index=True)\n",
    "print(f\"\\nCoachCox final: {len(coachcox):,} rows\")\n",
    "print(f\"  Finishers: {len(cc_finishers):,}, Non-finishers: {len(cc_nonfinishers):,}\")\n",
    "print(f\"  Gender: {coachcox['gender'].value_counts().to_dict()}\")\n",
    "\n",
    "del cc_raw, cc_races, cc_series, cc_races_enriched\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load T100 Pro Results — ~500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 T100 files\n",
      "  dubai-t100-2024.csv: 36 rows\n",
      "  dubai-t100-2025.csv: 34 rows\n",
      "  french-riviera-t100-2025.csv: 38 rows\n",
      "  ibiza-t100-2024.csv: 37 rows\n",
      "  london-t100-2024.csv: 38 rows\n",
      "  london-t100-2025.csv: 34 rows\n",
      "  miami-t100-2024.csv: 27 rows\n",
      "  qatar-t100-2025.csv: 44 rows\n",
      "  san-francisco-t100-2024.csv: 34 rows\n",
      "  san-francisco-t100-2025.csv: 40 rows\n",
      "  singapore-t100-2024.csv: 32 rows\n",
      "  singapore-t100-2025.csv: 32 rows\n",
      "  spain-t100-2025.csv: 34 rows\n",
      "  vancouver-t100-2025.csv: 36 rows\n",
      "\n",
      "T100 combined: 496 rows\n",
      "  [t100] Quality filter: 496 → 454 (42 removed)\n",
      "    - sum tolerance: 42\n",
      "T100 final: 454 rows\n"
     ]
    }
   ],
   "source": [
    "# Load all T100 CSV files\n",
    "t100_files = sorted(glob.glob(str(SCRAPED_DIR / 't100' / '*.csv')))\n",
    "print(f\"Found {len(t100_files)} T100 files\")\n",
    "\n",
    "t100_dfs = []\n",
    "for f in t100_files:\n",
    "    df = pd.read_csv(f)\n",
    "    t100_dfs.append(df)\n",
    "    print(f\"  {Path(f).name}: {len(df)} rows\")\n",
    "\n",
    "t100 = pd.concat(t100_dfs, ignore_index=True)\n",
    "print(f\"\\nT100 combined: {len(t100):,} rows\")\n",
    "\n",
    "# T100 is already in the target schema, just need minor adjustments\n",
    "t100_out = pd.DataFrame({\n",
    "    'athlete_hash': t100.apply(\n",
    "        lambda r: hash_athlete(r['athlete_name'], r.get('country', ''), r.get('gender', '')), axis=1\n",
    "    ),\n",
    "    'athlete_name': t100['athlete_name'],\n",
    "    'gender': t100['gender'],\n",
    "    'age_group': t100.get('age_group', pd.Series(dtype=str)),\n",
    "    'age_band': np.nan,\n",
    "    'country': t100.get('country', pd.Series(dtype=str)),\n",
    "    'country_iso2': np.nan,\n",
    "    'is_pro': True,  # All T100 are pro\n",
    "    'event_name': t100['event_name'],\n",
    "    'event_year': t100['event_year'],\n",
    "    'event_location': t100['event_name'].str.replace(' T100', '').str.replace(' t100', ''),\n",
    "    'event_distance': t100['event_distance'],\n",
    "    'source': 't100',\n",
    "    'swim_sec': t100['swim_sec'].astype(float),\n",
    "    't1_sec': t100['t1_sec'].astype(float),\n",
    "    'bike_sec': t100['bike_sec'].astype(float),\n",
    "    't2_sec': t100['t2_sec'].astype(float),\n",
    "    'run_sec': t100['run_sec'].astype(float),\n",
    "    'total_sec': t100['total_sec'].astype(float),\n",
    "    'overall_rank': pd.to_numeric(t100.get('overall_rank'), errors='coerce'),\n",
    "    'gender_rank': pd.to_numeric(t100.get('gender_rank'), errors='coerce'),\n",
    "    'age_group_rank': pd.to_numeric(t100.get('age_group_rank'), errors='coerce'),\n",
    "    'division_rank': np.nan,\n",
    "    'finish_status': 'finisher',\n",
    "})\n",
    "\n",
    "t100_out = apply_quality_filters(t100_out, label='t100')\n",
    "print(f\"T100 final: {len(t100_out):,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Wiki World Championship Results — 49 JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 Wiki JSON files\n",
      "  challenge-roth.json: 40 records\n",
      "  ironman-703-wc-2007.json: 158 records\n",
      "  ironman-703-wc-2008.json: 194 records\n",
      "  ironman-703-wc-2009.json: 221 records\n",
      "  ironman-703-wc-2010.json: 254 records\n",
      "  ironman-703-wc-2011.json: 248 records\n",
      "  ironman-703-wc-2012.json: 362 records\n",
      "  ironman-703-wc-2013.json: 374 records\n",
      "  ironman-703-wc-2014.json: 20 records\n",
      "  ironman-703-wc-2015.json: 20 records\n",
      "  ironman-703-wc-2016.json: 20 records\n",
      "  ironman-703-wc-2017.json: 20 records\n",
      "  ironman-hawaii.json: 282 records\n",
      "  ironman-wc-2005.json: 20 records\n",
      "  ironman-wc-2006.json: 20 records\n",
      "  ironman-wc-2007.json: 20 records\n",
      "  ironman-wc-2008.json: 146 records\n",
      "  ironman-wc-2009.json: 151 records\n",
      "  ironman-wc-2010.json: 164 records\n",
      "  ironman-wc-2011.json: 158 records\n",
      "  ironman-wc-2012.json: 194 records\n",
      "  ironman-wc-2013.json: 188 records\n",
      "  ironman-wc-2014.json: 20 records\n",
      "  ironman-wc-2015.json: 20 records\n",
      "  ironman-wc-2016.json: 20 records\n",
      "  ironman-wc-2017.json: 20 records\n",
      "  ironman-wc-2018.json: 20 records\n",
      "  ironman-wc-2019.json: 20 records\n",
      "  ironman-wc-2021.json: 282 records\n",
      "  ironman-wc-2022.json: 282 records\n",
      "  ironman-wc-2023.json: 282 records\n",
      "  ironman-wc-2024.json: 282 records\n",
      "  ironman-wc.json: 282 records\n",
      "  olympics-2000-men.json: 48 records\n",
      "  olympics-2000-women.json: 40 records\n",
      "  olympics-2004-men.json: 45 records\n",
      "  olympics-2004-women.json: 44 records\n",
      "  olympics-2008-men.json: 50 records\n",
      "  olympics-2008-women.json: 45 records\n",
      "  olympics-2012-men.json: 54 records\n",
      "  olympics-2012-women.json: 52 records\n",
      "  olympics-2016-men.json: 50 records\n",
      "  olympics-2016-women.json: 48 records\n",
      "  olympics-2020-men.json: 48 records\n",
      "  olympics-2020-relay.json: 15 records\n",
      "  olympics-2020-women.json: 34 records\n",
      "  olympics-2024-men.json: 50 records\n",
      "  olympics-2024-relay.json: 14 records\n",
      "  olympics-2024-women.json: 51 records\n",
      "\n",
      "Wiki combined: 5,492 rows\n",
      "Columns: ['athlete_name', 'gender', 'age_group', 'country', 'swim_sec', 'bike_sec', 'run_sec', 't1_sec', 't2_sec', 'total_sec', 'overall_rank', 'age_group_rank', 'gender_rank', 'event_name', 'event_year', 'event_distance', 'source_url']\n",
      "Sample event_distance values: <StringArray>\n",
      "['140.6', '70.3', 'olympic', 'olympic-relay']\n",
      "Length: 4, dtype: str\n"
     ]
    }
   ],
   "source": [
    "# Load all Wiki JSON files\n",
    "wiki_files = sorted(glob.glob(str(SCRAPED_DIR / 'wiki' / '*.json')))\n",
    "print(f\"Found {len(wiki_files)} Wiki JSON files\")\n",
    "\n",
    "wiki_records = []\n",
    "for f in wiki_files:\n",
    "    with open(f) as fh:\n",
    "        data = json.load(fh)\n",
    "    records = data.get('records', [])\n",
    "    for r in records:\n",
    "        wiki_records.append(r)\n",
    "    if records:\n",
    "        print(f\"  {Path(f).name}: {len(records)} records\")\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_records)\n",
    "print(f\"\\nWiki combined: {len(wiki_df):,} rows\")\n",
    "if len(wiki_df) > 0:\n",
    "    print(f\"Columns: {list(wiki_df.columns)}\")\n",
    "    print(f\"Sample event_distance values: {wiki_df['event_distance'].unique()[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [wiki] Quality filter: 5,492 → 5,488 (4 removed)\n",
      "    - total_sec bounds: 4\n",
      "    - swim_sec pct bounds: 1\n",
      "    - bike_sec pct bounds: 1\n",
      "    - run_sec pct bounds: 1\n",
      "Wiki final: 5,488 rows\n"
     ]
    }
   ],
   "source": [
    "# Normalize Wiki data\n",
    "if len(wiki_df) > 0:\n",
    "    wiki_out = pd.DataFrame({\n",
    "        'athlete_hash': wiki_df.apply(\n",
    "            lambda r: hash_athlete(r.get('athlete_name'), r.get('country', ''), r.get('gender', '')), axis=1\n",
    "        ),\n",
    "        'athlete_name': wiki_df.get('athlete_name'),\n",
    "        'gender': wiki_df.get('gender', pd.Series(dtype=str)).apply(\n",
    "            lambda x: normalize_gender(x) if pd.notna(x) and str(x).strip() else np.nan\n",
    "        ),\n",
    "        'age_group': wiki_df.get('age_group', pd.Series(dtype=str)),\n",
    "        'age_band': np.nan,\n",
    "        'country': wiki_df.get('country'),\n",
    "        'country_iso2': np.nan,\n",
    "        'is_pro': True,  # Wiki results are championship podiums\n",
    "        'event_name': wiki_df.get('event_name'),\n",
    "        'event_year': pd.to_numeric(wiki_df.get('event_year'), errors='coerce'),\n",
    "        'event_location': wiki_df.get('event_name'),\n",
    "        'event_distance': wiki_df.get('event_distance'),\n",
    "        'source': 'wiki',\n",
    "        'swim_sec': pd.to_numeric(wiki_df.get('swim_sec'), errors='coerce'),\n",
    "        't1_sec': pd.to_numeric(wiki_df.get('t1_sec'), errors='coerce'),\n",
    "        'bike_sec': pd.to_numeric(wiki_df.get('bike_sec'), errors='coerce'),\n",
    "        't2_sec': pd.to_numeric(wiki_df.get('t2_sec'), errors='coerce'),\n",
    "        'run_sec': pd.to_numeric(wiki_df.get('run_sec'), errors='coerce'),\n",
    "        'total_sec': pd.to_numeric(wiki_df.get('total_sec'), errors='coerce'),\n",
    "        'overall_rank': pd.to_numeric(wiki_df.get('overall_rank'), errors='coerce'),\n",
    "        'gender_rank': pd.to_numeric(wiki_df.get('gender_rank'), errors='coerce'),\n",
    "        'age_group_rank': pd.to_numeric(wiki_df.get('age_group_rank'), errors='coerce'),\n",
    "        'division_rank': np.nan,\n",
    "        'finish_status': 'finisher',\n",
    "    })\n",
    "\n",
    "    # Drop rows without valid total\n",
    "    wiki_out = wiki_out.dropna(subset=['total_sec'])\n",
    "    wiki_out = apply_quality_filters(wiki_out, label='wiki')\n",
    "    print(f\"Wiki final: {len(wiki_out):,} rows\")\n",
    "else:\n",
    "    wiki_out = pd.DataFrame()\n",
    "    print(\"No wiki records found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combine All Sources into Unified DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all sources...\n",
      "\n",
      "Combined: 4,137,242 total rows\n",
      "\n",
      "Source breakdown:\n",
      "source\n",
      "tristat       2426245\n",
      "coachcox       864982\n",
      "kaggle_df6     840073\n",
      "wiki             5488\n",
      "t100              454\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distance breakdown:\n",
      "event_distance\n",
      "70.3             2205311\n",
      "140.6            1558492\n",
      "olympic           260350\n",
      "sprint            112606\n",
      "100km                454\n",
      "olympic-relay         29\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gender breakdown:\n",
      "gender\n",
      "M    3196793\n",
      "F     934774\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pro vs AG: 19,529 pro, 4,117,713 AG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all sources\n",
    "print(\"Combining all sources...\")\n",
    "all_dfs = [df6, tristat, coachcox, t100_out]\n",
    "if len(wiki_out) > 0:\n",
    "    all_dfs.append(wiki_out)\n",
    "\n",
    "df_unified = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"\\nCombined: {len(df_unified):,} total rows\")\n",
    "print(f\"\\nSource breakdown:\")\n",
    "print(df_unified['source'].value_counts())\n",
    "print(f\"\\nDistance breakdown:\")\n",
    "print(df_unified['event_distance'].value_counts())\n",
    "print(f\"\\nGender breakdown:\")\n",
    "print(df_unified['gender'].value_counts())\n",
    "print(f\"\\nPro vs AG: {df_unified['is_pro'].sum():,} pro, {(~df_unified['is_pro']).sum():,} AG\")\n",
    "\n",
    "# Free individual DataFrames\n",
    "del df6, tristat, coachcox, t100_out, wiki_out\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Derived Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing derived features...\n",
      "Split percentages computed.\n",
      "  swim_pct median: 0.109\n",
      "  bike_pct median: 0.506\n",
      "  run_pct median:  0.360\n"
     ]
    }
   ],
   "source": [
    "# ─── Split Percentages ─────────────────────────────────────────────\n",
    "print(\"Computing derived features...\")\n",
    "\n",
    "# Split ratios (as fraction of total)\n",
    "for seg in ['swim', 'bike', 'run']:\n",
    "    col = f'{seg}_sec'\n",
    "    df_unified[f'{seg}_pct'] = np.where(\n",
    "        (df_unified['total_sec'] > 0) & df_unified[col].notna(),\n",
    "        df_unified[col] / df_unified['total_sec'],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "# Transition percentage\n",
    "df_unified['transition_pct'] = np.where(\n",
    "    (df_unified['total_sec'] > 0) & df_unified['t1_sec'].notna() & df_unified['t2_sec'].notna(),\n",
    "    (df_unified['t1_sec'] + df_unified['t2_sec']) / df_unified['total_sec'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Bike-run ratio\n",
    "df_unified['bike_run_ratio'] = np.where(\n",
    "    (df_unified['run_sec'] > 0) & df_unified['bike_sec'].notna(),\n",
    "    df_unified['bike_sec'] / df_unified['run_sec'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "print(\"Split percentages computed.\")\n",
    "print(f\"  swim_pct median: {df_unified['swim_pct'].median():.3f}\")\n",
    "print(f\"  bike_pct median: {df_unified['bike_pct'].median():.3f}\")\n",
    "print(f\"  run_pct median:  {df_unified['run_pct'].median():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cohort statistics...\n",
      "Cohorts with ≥30 samples: 278\n",
      "Total athletes in valid cohorts: 3,835,792\n",
      "Records with cohort stats: 3,835,792\n"
     ]
    }
   ],
   "source": [
    "# ─── Cohort Statistics ─────────────────────────────────────────────\n",
    "# Compute median/std per cohort (gender × age_group × distance) for AG only\n",
    "print(\"Computing cohort statistics...\")\n",
    "\n",
    "# Filter to AG only for cohort computation\n",
    "ag_mask = ~df_unified['is_pro']\n",
    "cohort_cols = ['gender', 'age_group', 'event_distance']\n",
    "\n",
    "cohort_stats = df_unified[ag_mask].groupby(cohort_cols).agg(\n",
    "    cohort_median_total=('total_sec', 'median'),\n",
    "    cohort_std_total=('total_sec', 'std'),\n",
    "    cohort_median_swim=('swim_sec', 'median'),\n",
    "    cohort_median_bike=('bike_sec', 'median'),\n",
    "    cohort_median_run=('run_sec', 'median'),\n",
    "    cohort_count=('total_sec', 'count'),\n",
    ").reset_index()\n",
    "\n",
    "# Only keep cohorts with >= 30 samples\n",
    "cohort_stats = cohort_stats[cohort_stats['cohort_count'] >= 30]\n",
    "print(f\"Cohorts with ≥30 samples: {len(cohort_stats):,}\")\n",
    "print(f\"Total athletes in valid cohorts: {cohort_stats['cohort_count'].sum():,}\")\n",
    "\n",
    "# Merge back to main df\n",
    "df_unified = df_unified.merge(cohort_stats, on=cohort_cols, how='left')\n",
    "print(f\"Records with cohort stats: {df_unified['cohort_median_total'].notna().sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cohort percentiles...\n",
      "Fade ratio: median=1.000, std=0.209\n",
      "Cohort percentile: 3,835,792 records with percentile\n",
      "Implied bike IF: median=1.000\n"
     ]
    }
   ],
   "source": [
    "# ─── Fade Ratio & Cohort Percentile ────────────────────────────────\n",
    "# Fade ratio: actual run / cohort median run (> 1 means slower than typical)\n",
    "df_unified['fade_ratio'] = np.where(\n",
    "    (df_unified['cohort_median_run'] > 0) & df_unified['run_sec'].notna(),\n",
    "    df_unified['run_sec'] / df_unified['cohort_median_run'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Cohort percentile: rank within cohort (0-100, lower = faster)\n",
    "print(\"Computing cohort percentiles...\")\n",
    "df_unified['cohort_percentile'] = np.nan\n",
    "\n",
    "for (g, ag, d), group in df_unified.groupby(cohort_cols):\n",
    "    mask = group.index\n",
    "    valid = group['total_sec'].notna()\n",
    "    if valid.sum() >= 30:\n",
    "        ranks = group.loc[valid, 'total_sec'].rank(pct=True)\n",
    "        df_unified.loc[ranks.index, 'cohort_percentile'] = ranks.values\n",
    "\n",
    "# Implied bike intensity factor\n",
    "df_unified['implied_bike_if'] = np.where(\n",
    "    (df_unified['cohort_median_bike'] > 0) & df_unified['bike_sec'].notna(),\n",
    "    df_unified['bike_sec'] / df_unified['cohort_median_bike'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "print(f\"Fade ratio: median={df_unified['fade_ratio'].median():.3f}, std={df_unified['fade_ratio'].std():.3f}\")\n",
    "print(f\"Cohort percentile: {df_unified['cohort_percentile'].notna().sum():,} records with percentile\")\n",
    "print(f\"Implied bike IF: median={df_unified['implied_bike_if'].median():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication: 4,137,242 → 4,124,345 (12,897 duplicates removed)\n",
      "\n",
      "Final source breakdown:\n",
      "source\n",
      "tristat       2419629\n",
      "coachcox       864947\n",
      "kaggle_df6     833897\n",
      "wiki             5418\n",
      "t100              454\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Deduplication strategy:\n",
    "# 1. Within-source exact duplicates (same athlete + event + time)\n",
    "# 2. Cross-source: keep all records but flag potential duplicates\n",
    "\n",
    "n_before = len(df_unified)\n",
    "\n",
    "# Within-source dedup: drop exact duplicates on key columns\n",
    "dedup_cols = ['source', 'athlete_name', 'event_name', 'event_year', 'total_sec', 'gender', 'age_group']\n",
    "# For df6 (no names), dedup on all demographics + event + time\n",
    "df_unified = df_unified.drop_duplicates(\n",
    "    subset=[c for c in dedup_cols if c in df_unified.columns],\n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "n_after = len(df_unified)\n",
    "print(f\"Deduplication: {n_before:,} → {n_after:,} ({n_before - n_after:,} duplicates removed)\")\n",
    "print(f\"\\nFinal source breakdown:\")\n",
    "print(df_unified['source'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Athlete Profile Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building athlete profiles...\n",
      "Records with athlete identity: 3,286,137\n",
      "Unique athletes: 1,629,366\n",
      "Profiles built: 1,629,366\n",
      "Races distribution: count    1.629366e+06\n",
      "mean     2.016819e+00\n",
      "std      2.639530e+00\n",
      "min      1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      2.000000e+00\n",
      "max      4.220000e+02\n",
      "Name: total_races, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Build athlete profiles for athletes with known identity (hash != None)\n",
    "print(\"Building athlete profiles...\")\n",
    "\n",
    "# Filter to athletes with hash\n",
    "named_athletes = df_unified[df_unified['athlete_hash'].notna()].copy()\n",
    "print(f\"Records with athlete identity: {len(named_athletes):,}\")\n",
    "print(f\"Unique athletes: {named_athletes['athlete_hash'].nunique():,}\")\n",
    "\n",
    "# Aggregate by athlete_hash\n",
    "profiles = named_athletes.groupby('athlete_hash').agg(\n",
    "    athlete_name=('athlete_name', 'first'),\n",
    "    gender=('gender', 'first'),\n",
    "    country=('country', 'first'),\n",
    "    total_races=('total_sec', 'count'),\n",
    "    first_race_year=('event_year', 'min'),\n",
    "    latest_race_year=('event_year', 'max'),\n",
    "    pb_swim_sec=('swim_sec', 'min'),\n",
    "    pb_bike_sec=('bike_sec', 'min'),\n",
    "    pb_run_sec=('run_sec', 'min'),\n",
    "    pb_total_sec=('total_sec', 'min'),\n",
    "    avg_total_sec=('total_sec', 'mean'),\n",
    "    std_total_sec=('total_sec', 'std'),\n",
    "    avg_fade_ratio=('fade_ratio', 'mean'),\n",
    "    avg_swim_pct=('swim_pct', 'mean'),\n",
    "    avg_bike_pct=('bike_pct', 'mean'),\n",
    "    avg_run_pct=('run_pct', 'mean'),\n",
    ").reset_index()\n",
    "\n",
    "# Years active\n",
    "profiles['years_active'] = profiles['latest_race_year'] - profiles['first_race_year'] + 1\n",
    "\n",
    "# Distances raced (comma-separated list)\n",
    "dist_agg = named_athletes.groupby('athlete_hash')['event_distance'].apply(\n",
    "    lambda x: ','.join(sorted(x.dropna().unique()))\n",
    ").reset_index()\n",
    "dist_agg.columns = ['athlete_hash', 'distances_raced']\n",
    "profiles = profiles.merge(dist_agg, on='athlete_hash', how='left')\n",
    "\n",
    "# Consistency CV (coefficient of variation)\n",
    "profiles['consistency_cv'] = profiles['std_total_sec'] / profiles['avg_total_sec']\n",
    "profiles.loc[profiles['total_races'] < 2, 'consistency_cv'] = np.nan\n",
    "\n",
    "print(f\"Profiles built: {len(profiles):,}\")\n",
    "print(f\"Races distribution: {profiles['total_races'].describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing improvement slopes (this may take a minute)...\n",
      "Improvement slopes computed. Median: 0.0 sec/year\n"
     ]
    }
   ],
   "source": [
    "# ─── Improvement Slope (linear fit of total_sec over years) ────────\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "def compute_improvement_slope(group):\n",
    "    \"\"\"Compute linear regression slope of total_sec vs event_year.\"\"\"\n",
    "    valid = group.dropna(subset=['event_year', 'total_sec'])\n",
    "    if len(valid) < 2:\n",
    "        return np.nan\n",
    "    years = valid['event_year'].values.astype(float)\n",
    "    times = valid['total_sec'].values\n",
    "    if years.std() == 0:\n",
    "        return 0.0\n",
    "    slope, _, _, _, _ = scipy_stats.linregress(years, times)\n",
    "    return slope  # negative = improving\n",
    "\n",
    "print(\"Computing improvement slopes (this may take a minute)...\")\n",
    "slopes = named_athletes.groupby('athlete_hash').apply(compute_improvement_slope)\n",
    "slopes.name = 'improvement_slope'\n",
    "profiles = profiles.merge(slopes.reset_index(), on='athlete_hash', how='left')\n",
    "print(f\"Improvement slopes computed. Median: {profiles['improvement_slope'].median():.1f} sec/year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discipline z-scores computed.\n",
      "Dominant discipline distribution:\n",
      "dominant_discipline\n",
      "swim    624169\n",
      "run     460124\n",
      "bike    455819\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ─── Discipline Strength Z-Scores ──────────────────────────────────\n",
    "# For each athlete, compute their avg segment time relative to their cohort\n",
    "\n",
    "# Get the most common cohort for each athlete\n",
    "cohort_lookup = named_athletes.groupby('athlete_hash').agg({\n",
    "    'gender': 'first',\n",
    "    'age_group': lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan,\n",
    "    'event_distance': lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan,\n",
    "}).reset_index()\n",
    "\n",
    "# Merge cohort stats\n",
    "cohort_lookup = cohort_lookup.merge(\n",
    "    cohort_stats[['gender', 'age_group', 'event_distance',\n",
    "                  'cohort_median_swim', 'cohort_median_bike', 'cohort_median_run',\n",
    "                  'cohort_std_total']],\n",
    "    on=['gender', 'age_group', 'event_distance'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Get athlete average times\n",
    "athlete_avg = named_athletes.groupby('athlete_hash').agg({\n",
    "    'swim_sec': 'mean',\n",
    "    'bike_sec': 'mean',\n",
    "    'run_sec': 'mean',\n",
    "}).reset_index()\n",
    "athlete_avg.columns = ['athlete_hash', 'avg_swim', 'avg_bike', 'avg_run']\n",
    "\n",
    "z_data = cohort_lookup.merge(athlete_avg, on='athlete_hash', how='left')\n",
    "\n",
    "# Z-scores (negative = faster than cohort median = STRONGER)\n",
    "z_data['swim_strength_z'] = -(z_data['avg_swim'] - z_data['cohort_median_swim']) / z_data['cohort_std_total'].clip(lower=300)\n",
    "z_data['bike_strength_z'] = -(z_data['avg_bike'] - z_data['cohort_median_bike']) / z_data['cohort_std_total'].clip(lower=300)\n",
    "z_data['run_strength_z'] = -(z_data['avg_run'] - z_data['cohort_median_run']) / z_data['cohort_std_total'].clip(lower=300)\n",
    "\n",
    "# Dominant discipline\n",
    "def get_dominant(row):\n",
    "    vals = {'swim': row.get('swim_strength_z', 0), 'bike': row.get('bike_strength_z', 0), 'run': row.get('run_strength_z', 0)}\n",
    "    if all(pd.isna(v) for v in vals.values()):\n",
    "        return np.nan\n",
    "    return max(vals, key=lambda k: vals[k] if pd.notna(vals[k]) else -999)\n",
    "\n",
    "z_data['dominant_discipline'] = z_data.apply(get_dominant, axis=1)\n",
    "\n",
    "# Merge z-scores into profiles\n",
    "profiles = profiles.merge(\n",
    "    z_data[['athlete_hash', 'swim_strength_z', 'bike_strength_z', 'run_strength_z', 'dominant_discipline']],\n",
    "    on='athlete_hash', how='left'\n",
    ")\n",
    "\n",
    "print(\"Discipline z-scores computed.\")\n",
    "print(f\"Dominant discipline distribution:\\n{profiles['dominant_discipline'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNF stats added. Athletes with DNF > 0: 747\n",
      "\n",
      "Final profile shape: (1629366, 22)\n",
      "Profile columns: ['athlete_hash', 'athlete_name', 'gender', 'country', 'total_races', 'first_race_year', 'latest_race_year', 'pb_swim_sec', 'pb_bike_sec', 'pb_run_sec', 'pb_total_sec', 'avg_fade_ratio', 'years_active', 'distances_raced', 'consistency_cv', 'improvement_slope', 'swim_strength_z', 'bike_strength_z', 'run_strength_z', 'dominant_discipline', 'dnf_count', 'dnf_rate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>athlete_hash</th>\n",
       "      <th>athlete_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>country</th>\n",
       "      <th>total_races</th>\n",
       "      <th>first_race_year</th>\n",
       "      <th>latest_race_year</th>\n",
       "      <th>pb_swim_sec</th>\n",
       "      <th>pb_bike_sec</th>\n",
       "      <th>pb_run_sec</th>\n",
       "      <th>pb_total_sec</th>\n",
       "      <th>avg_fade_ratio</th>\n",
       "      <th>years_active</th>\n",
       "      <th>distances_raced</th>\n",
       "      <th>consistency_cv</th>\n",
       "      <th>improvement_slope</th>\n",
       "      <th>swim_strength_z</th>\n",
       "      <th>bike_strength_z</th>\n",
       "      <th>run_strength_z</th>\n",
       "      <th>dominant_discipline</th>\n",
       "      <th>dnf_count</th>\n",
       "      <th>dnf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002270c5ed2e96</td>\n",
       "      <td>Хабиев Данияр</td>\n",
       "      <td>M</td>\n",
       "      <td>RUS</td>\n",
       "      <td>7</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1178.0</td>\n",
       "      <td>2555.0</td>\n",
       "      <td>1597.0</td>\n",
       "      <td>5537.0</td>\n",
       "      <td>1.143262</td>\n",
       "      <td>7.0</td>\n",
       "      <td>70.3,olympic,sprint</td>\n",
       "      <td>0.516717</td>\n",
       "      <td>-772.818750</td>\n",
       "      <td>-0.371914</td>\n",
       "      <td>-0.853019</td>\n",
       "      <td>-0.604193</td>\n",
       "      <td>swim</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000028766426829f</td>\n",
       "      <td>Mark Van Der Pol</td>\n",
       "      <td>M</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>5</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>4262.0</td>\n",
       "      <td>21797.0</td>\n",
       "      <td>17721.0</td>\n",
       "      <td>45224.0</td>\n",
       "      <td>1.088875</td>\n",
       "      <td>13.0</td>\n",
       "      <td>140.6</td>\n",
       "      <td>0.043329</td>\n",
       "      <td>201.755102</td>\n",
       "      <td>0.011835</td>\n",
       "      <td>-0.309085</td>\n",
       "      <td>-0.284992</td>\n",
       "      <td>swim</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003fdcfe8312ba</td>\n",
       "      <td>Chipping, James</td>\n",
       "      <td>M</td>\n",
       "      <td>GBR</td>\n",
       "      <td>2</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>10242.0</td>\n",
       "      <td>9207.0</td>\n",
       "      <td>21980.0</td>\n",
       "      <td>1.202657</td>\n",
       "      <td>3.0</td>\n",
       "      <td>140.6,70.3</td>\n",
       "      <td>0.466142</td>\n",
       "      <td>10807.000000</td>\n",
       "      <td>0.242420</td>\n",
       "      <td>0.833100</td>\n",
       "      <td>0.406317</td>\n",
       "      <td>bike</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       athlete_hash      athlete_name gender      country  total_races  first_race_year  latest_race_year  pb_swim_sec  pb_bike_sec  pb_run_sec  pb_total_sec  avg_fade_ratio  years_active  \\\n",
       "0  00002270c5ed2e96     Хабиев Данияр      M          RUS            7           2015.0            2021.0       1178.0       2555.0      1597.0        5537.0        1.143262           7.0   \n",
       "1  000028766426829f  Mark Van Der Pol      M  Netherlands            5           2012.0            2024.0       4262.0      21797.0     17721.0       45224.0        1.088875          13.0   \n",
       "2  00003fdcfe8312ba   Chipping, James      M          GBR            2           2016.0            2018.0       1921.0      10242.0      9207.0       21980.0        1.202657           3.0   \n",
       "\n",
       "       distances_raced  consistency_cv  improvement_slope  swim_strength_z  bike_strength_z  run_strength_z dominant_discipline  dnf_count  dnf_rate  \n",
       "0  70.3,olympic,sprint        0.516717        -772.818750        -0.371914        -0.853019       -0.604193                swim          0       0.0  \n",
       "1                140.6        0.043329         201.755102         0.011835        -0.309085       -0.284992                swim          0       0.0  \n",
       "2           140.6,70.3        0.466142       10807.000000         0.242420         0.833100        0.406317                bike          0       0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─── DNF Rate ──────────────────────────────────────────────────────\n",
    "# Only from CoachCox which has finishStatus\n",
    "dnf_data = named_athletes[named_athletes['source'] == 'coachcox'].copy()\n",
    "if len(dnf_data) > 0:\n",
    "    dnf_agg = dnf_data.groupby('athlete_hash').agg(\n",
    "        dnf_count=('finish_status', lambda x: (x != 'finisher').sum()),\n",
    "        total_cc_races=('finish_status', 'count'),\n",
    "    ).reset_index()\n",
    "    dnf_agg['dnf_rate'] = dnf_agg['dnf_count'] / dnf_agg['total_cc_races']\n",
    "\n",
    "    profiles = profiles.merge(dnf_agg[['athlete_hash', 'dnf_count', 'dnf_rate']], on='athlete_hash', how='left')\n",
    "    profiles['dnf_count'] = profiles['dnf_count'].fillna(0).astype(int)\n",
    "    profiles['dnf_rate'] = profiles['dnf_rate'].fillna(0)\n",
    "    print(f\"DNF stats added. Athletes with DNF > 0: {(profiles['dnf_count'] > 0).sum():,}\")\n",
    "else:\n",
    "    profiles['dnf_count'] = 0\n",
    "    profiles['dnf_rate'] = 0.0\n",
    "\n",
    "# Clean up temporary columns\n",
    "profiles = profiles.drop(columns=['avg_total_sec', 'std_total_sec', 'avg_swim_pct', 'avg_bike_pct', 'avg_run_pct'], errors='ignore')\n",
    "\n",
    "print(f\"\\nFinal profile shape: {profiles.shape}\")\n",
    "print(f\"Profile columns: {list(profiles.columns)}\")\n",
    "profiles.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved athlete_race.csv: 4,124,345 rows → /Users/mykyta/projects/indie/racedayai/research/data/cleaned/athlete_race.csv\n",
      "File size: 1201.5 MB\n"
     ]
    }
   ],
   "source": [
    "# ─── Save athlete_race.csv ─────────────────────────────────────────\n",
    "# Select final columns in order\n",
    "race_columns = [\n",
    "    'athlete_hash', 'athlete_name', 'gender', 'age_group', 'age_band',\n",
    "    'country', 'country_iso2', 'is_pro',\n",
    "    'event_name', 'event_year', 'event_location', 'event_distance', 'source',\n",
    "    'swim_sec', 't1_sec', 'bike_sec', 't2_sec', 'run_sec', 'total_sec',\n",
    "    'overall_rank', 'gender_rank', 'age_group_rank', 'division_rank', 'finish_status',\n",
    "    'swim_pct', 'bike_pct', 'run_pct', 'transition_pct',\n",
    "    'bike_run_ratio', 'fade_ratio', 'cohort_percentile', 'implied_bike_if',\n",
    "]\n",
    "\n",
    "# Keep only columns that exist\n",
    "race_columns = [c for c in race_columns if c in df_unified.columns]\n",
    "df_out = df_unified[race_columns].copy()\n",
    "\n",
    "# Drop cohort helper columns\n",
    "df_out = df_out.drop(columns=[c for c in df_out.columns if c.startswith('cohort_median_') or c.startswith('cohort_std_') or c == 'cohort_count'], errors='ignore')\n",
    "\n",
    "output_path = CLEANED_DIR / 'athlete_race.csv'\n",
    "df_out.to_csv(output_path, index=False)\n",
    "print(f\"Saved athlete_race.csv: {len(df_out):,} rows → {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1024 / 1024:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved athlete_profile.csv: 1,629,366 rows → /Users/mykyta/projects/indie/racedayai/research/data/cleaned/athlete_profile.csv\n",
      "File size: 305.6 MB\n"
     ]
    }
   ],
   "source": [
    "# ─── Save athlete_profile.csv ──────────────────────────────────────\n",
    "profile_columns = [\n",
    "    'athlete_hash', 'athlete_name', 'gender', 'country',\n",
    "    'total_races', 'years_active', 'distances_raced',\n",
    "    'pb_swim_sec', 'pb_bike_sec', 'pb_run_sec', 'pb_total_sec',\n",
    "    'first_race_year', 'latest_race_year',\n",
    "    'improvement_slope', 'consistency_cv',\n",
    "    'swim_strength_z', 'bike_strength_z', 'run_strength_z',\n",
    "    'dominant_discipline',\n",
    "    'dnf_count', 'dnf_rate', 'avg_fade_ratio',\n",
    "]\n",
    "\n",
    "profile_columns = [c for c in profile_columns if c in profiles.columns]\n",
    "profiles_out = profiles[profile_columns].copy()\n",
    "\n",
    "output_path = CLEANED_DIR / 'athlete_profile.csv'\n",
    "profiles_out.to_csv(output_path, index=False)\n",
    "print(f\"Saved athlete_profile.csv: {len(profiles_out):,} rows → {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / 1024 / 1024:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ETL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "athlete_race.csv:\n",
      "  Total rows:      4,124,345\n",
      "  Sources:          {'tristat': 2419629, 'coachcox': 864947, 'kaggle_df6': 833897, 'wiki': 5418, 't100': 454}\n",
      "  Distances:        {'70.3': 2198944, '140.6': 1558226, 'olympic': 257249, 'sprint': 109443, '100km': 454, 'olympic-relay': 29}\n",
      "  Gender:           {'M': 3188641, 'F': 930099}\n",
      "  Pro athletes:     19,459 (0.5%)\n",
      "  Year range:       1978 - 2025\n",
      "  Finish status:    {'finisher': 4123587, 'dq': 310, 'dns': 267, 'dnf': 181}\n",
      "\n",
      "athlete_profile.csv:\n",
      "  Total athletes:   1,629,366\n",
      "  With 3+ races:    314,034\n",
      "  With 5+ races:    124,927\n",
      "  Median races:     1\n",
      "  Gender:           {'M': 1267791, 'F': 360620}\n",
      "\n",
      "Derived features (athlete_race.csv):\n",
      "  swim_pct                 : median=0.109, valid=97.0%\n",
      "  bike_pct                 : median=0.506, valid=98.6%\n",
      "  run_pct                  : median=0.360, valid=98.8%\n",
      "  fade_ratio               : median=1.000, valid=92.2%\n",
      "  cohort_percentile        : median=0.500, valid=92.8%\n",
      "  bike_run_ratio           : median=1.409, valid=98.4%\n",
      "\n",
      "Derived features (athlete_profile.csv):\n",
      "  improvement_slope        : median=0.000, valid=36.6%\n",
      "  consistency_cv           : median=0.079, valid=36.9%\n",
      "  swim_strength_z          : median=-0.021, valid=92.9%\n",
      "  bike_strength_z          : median=-0.094, valid=94.2%\n",
      "  run_strength_z           : median=-0.083, valid=94.3%\n",
      "\n",
      "✅ ETL complete. Ready for Notebook 02.\n"
     ]
    }
   ],
   "source": [
    "# ─── Final Summary ─────────────────────────────────────────────────\n",
    "print(\"=\" * 70)\n",
    "print(\"ETL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nathlete_race.csv:\")\n",
    "print(f\"  Total rows:      {len(df_out):,}\")\n",
    "print(f\"  Sources:          {df_out['source'].value_counts().to_dict()}\")\n",
    "print(f\"  Distances:        {df_out['event_distance'].value_counts().to_dict()}\")\n",
    "print(f\"  Gender:           {df_out['gender'].value_counts().to_dict()}\")\n",
    "print(f\"  Pro athletes:     {df_out['is_pro'].sum():,} ({df_out['is_pro'].mean()*100:.1f}%)\")\n",
    "print(f\"  Year range:       {df_out['event_year'].min():.0f} - {df_out['event_year'].max():.0f}\")\n",
    "print(f\"  Finish status:    {df_out['finish_status'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nathlete_profile.csv:\")\n",
    "print(f\"  Total athletes:   {len(profiles_out):,}\")\n",
    "print(f\"  With 3+ races:    {(profiles_out['total_races'] >= 3).sum():,}\")\n",
    "print(f\"  With 5+ races:    {(profiles_out['total_races'] >= 5).sum():,}\")\n",
    "print(f\"  Median races:     {profiles_out['total_races'].median():.0f}\")\n",
    "print(f\"  Gender:           {profiles_out['gender'].value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nDerived features (athlete_race.csv):\")\n",
    "for col in ['swim_pct', 'bike_pct', 'run_pct', 'fade_ratio', 'cohort_percentile', 'bike_run_ratio']:\n",
    "    if col in df_out.columns:\n",
    "        valid_pct = df_out[col].notna().mean() * 100\n",
    "        print(f\"  {col:25s}: median={df_out[col].median():.3f}, valid={valid_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nDerived features (athlete_profile.csv):\")\n",
    "for col in ['improvement_slope', 'consistency_cv', 'swim_strength_z', 'bike_strength_z', 'run_strength_z']:\n",
    "    if col in profiles_out.columns:\n",
    "        valid_pct = profiles_out[col].notna().mean() * 100\n",
    "        print(f\"  {col:25s}: median={profiles_out[col].median():.3f}, valid={valid_pct:.1f}%\")\n",
    "\n",
    "print(\"\\n✅ ETL complete. Ready for Notebook 02.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf9f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
