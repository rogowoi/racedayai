{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3824f9b2",
   "metadata": {},
   "source": [
    "# Notebook 05 — Neural Embeddings\n",
    "**RaceDayAI ML Prediction Engine (Plan 07)**\n",
    "\n",
    "PyTorch MLP embedding network. Multi-task learning: time prediction + split ratios + DNF risk.\n",
    "\"Athletes like you\" similarity search via cosine distance in embedding space.\n",
    "\n",
    "**Why combined (not per-distance)?** The embedding space is designed for cross-distance athlete\n",
    "similarity — a 70.3 athlete should be able to find comparable 140.6 athletes. Distance is an\n",
    "input feature (embedding dimension), not a data partition. Per-distance evaluation is reported.\n",
    "\n",
    "**Reads:** `athlete_race.csv`, `athlete_profile.csv`\n",
    "**Writes:** `athlete_embeddings.csv`, `neural_predictions.csv`, trained PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45ac592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 2.10.0, CUDA: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(f\"PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "BASE = Path('.').resolve().parent\n",
    "CLEANED = BASE / 'data' / 'cleaned'\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "MODEL_DISTANCES = ['70.3', '140.6']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d80b8",
   "metadata": {},
   "source": [
    "## 1. Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099e8675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  70.3: 2,197,121 records\n",
      "  140.6: 1,541,692 records\n",
      "Total: 3,738,813\n",
      "Categoricals: gender=2, age=162, country=51, dist=2\n",
      "Continuous features: 5\n",
      "Targets: total_sec (regression), split_pcts (3-way), is_dnf (binary)\n"
     ]
    }
   ],
   "source": [
    "races = pd.read_csv(CLEANED / 'athlete_race.csv', low_memory=False)\n",
    "profiles = pd.read_csv(CLEANED / 'athlete_profile.csv', low_memory=False)\n",
    "\n",
    "# Filter: AG, valid times, 70.3 + 140.6\n",
    "df = races[(races['is_pro'] != True) &\n",
    "           races['event_distance'].isin(MODEL_DISTANCES) &\n",
    "           races['total_sec'].notna() &\n",
    "           (races['total_sec'] > 3600) & (races['total_sec'] < 61200)].copy()\n",
    "\n",
    "for d in MODEL_DISTANCES:\n",
    "    n = (df['event_distance'] == d).sum()\n",
    "    print(f\"  {d}: {n:,} records\")\n",
    "print(f\"Total: {len(df):,}\")\n",
    "\n",
    "# Encode categoricals\n",
    "gender_map = {'M': 0, 'F': 1}\n",
    "df['gender_idx'] = df['gender'].map(gender_map).fillna(0).astype(int)\n",
    "N_GENDER = 2\n",
    "\n",
    "# Age group\n",
    "age_groups = sorted(df['age_group'].dropna().unique())\n",
    "age_map = {a: i for i, a in enumerate(age_groups)}\n",
    "df['age_idx'] = df['age_group'].map(age_map).fillna(0).astype(int)\n",
    "N_AGE = len(age_groups)\n",
    "\n",
    "# Country (top 50, rest = 0)\n",
    "country_counts = df['country'].value_counts()\n",
    "top_countries = country_counts.head(50).index.tolist()\n",
    "country_map = {c: i+1 for i, c in enumerate(top_countries)}\n",
    "df['country_idx'] = df['country'].map(country_map).fillna(0).astype(int)\n",
    "N_COUNTRY = len(top_countries) + 1\n",
    "\n",
    "# Distance (as embedding)\n",
    "dist_map = {'70.3': 0, '140.6': 1}\n",
    "df['dist_idx'] = df['event_distance'].map(dist_map).fillna(0).astype(int)\n",
    "N_DIST = 2\n",
    "\n",
    "# Continuous features (normalize)\n",
    "cont_features = ['swim_pct', 'bike_pct', 'run_pct', 'fade_ratio', 'bike_run_ratio']\n",
    "for col in cont_features:\n",
    "    if col in df.columns:\n",
    "        median = df[col].median()\n",
    "        df[col] = df[col].fillna(median)\n",
    "    else:\n",
    "        df[col] = 0.0\n",
    "\n",
    "# Per-distance normalization for continuous features\n",
    "# This accounts for different distributions between 70.3 and 140.6\n",
    "cont_stats = {}\n",
    "for d in MODEL_DISTANCES:\n",
    "    mask = df['event_distance'] == d\n",
    "    cont_stats[d] = {\n",
    "        'mean': df.loc[mask, cont_features].mean(),\n",
    "        'std': df.loc[mask, cont_features].std() + 1e-8,\n",
    "    }\n",
    "\n",
    "# Normalize using overall stats (since we're training one combined model)\n",
    "cont_means = df[cont_features].mean()\n",
    "cont_stds = df[cont_features].std() + 1e-8\n",
    "for col in cont_features:\n",
    "    df[f'{col}_norm'] = (df[col] - cont_means[col]) / cont_stds[col]\n",
    "\n",
    "cont_norm_cols = [f'{c}_norm' for c in cont_features]\n",
    "\n",
    "# Per-distance target normalization\n",
    "total_stats = {}\n",
    "for d in MODEL_DISTANCES:\n",
    "    mask = df['event_distance'] == d\n",
    "    total_stats[d] = {'mean': df.loc[mask, 'total_sec'].mean(),\n",
    "                       'std': df.loc[mask, 'total_sec'].std()}\n",
    "\n",
    "# Global normalization for the combined model\n",
    "total_mean = df['total_sec'].mean()\n",
    "total_std = df['total_sec'].std()\n",
    "df['total_norm'] = (df['total_sec'] - total_mean) / total_std\n",
    "\n",
    "# Split ratios as target\n",
    "# DNF flag (0/1)\n",
    "df['is_dnf'] = 0.0\n",
    "if 'finish_status' in df.columns:\n",
    "    df['is_dnf'] = df['finish_status'].fillna('').str.upper().isin(['DNF', 'DNS', 'DQ']).astype(float)\n",
    "\n",
    "print(f\"Categoricals: gender={N_GENDER}, age={N_AGE}, country={N_COUNTRY}, dist={N_DIST}\")\n",
    "print(f\"Continuous features: {len(cont_features)}\")\n",
    "print(f\"Targets: total_sec (regression), split_pcts (3-way), is_dnf (binary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3017bc",
   "metadata": {},
   "source": [
    "## 2. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be290dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2,869,556 | Val: 434,517 | Test: 434,740\n",
      "  70.3: train=1,789,393  test=203,608\n",
      "  140.6: train=1,080,163  test=231,132\n"
     ]
    }
   ],
   "source": [
    "class TriathlonDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.gender = torch.LongTensor(df['gender_idx'].values)\n",
    "        self.age = torch.LongTensor(df['age_idx'].values)\n",
    "        self.country = torch.LongTensor(df['country_idx'].values)\n",
    "        self.distance = torch.LongTensor(df['dist_idx'].values)\n",
    "        self.continuous = torch.FloatTensor(df[cont_norm_cols].values)\n",
    "        self.total = torch.FloatTensor(df['total_norm'].values)\n",
    "        self.splits = torch.FloatTensor(df[['swim_pct', 'bike_pct', 'run_pct']].fillna(0.33).values)\n",
    "        self.dnf = torch.FloatTensor(df['is_dnf'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gender)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'gender': self.gender[idx],\n",
    "            'age': self.age[idx],\n",
    "            'country': self.country[idx],\n",
    "            'distance': self.distance[idx],\n",
    "            'continuous': self.continuous[idx],\n",
    "            'total': self.total[idx],\n",
    "            'splits': self.splits[idx],\n",
    "            'dnf': self.dnf[idx],\n",
    "        }\n",
    "\n",
    "# Random split (grouped by athlete to prevent leakage)\n",
    "def random_athlete_split(data, train_frac=0.70, val_frac=0.15, seed=42):\n",
    "    athletes = data['athlete_hash'].unique()\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(athletes)\n",
    "    n = len(athletes)\n",
    "    n_train = int(train_frac * n)\n",
    "    n_val = int(val_frac * n)\n",
    "    train_ath = set(athletes[:n_train])\n",
    "    val_ath = set(athletes[n_train:n_train + n_val])\n",
    "    test_ath = set(athletes[n_train + n_val:])\n",
    "    return (data[data['athlete_hash'].isin(train_ath)].copy(),\n",
    "            data[data['athlete_hash'].isin(val_ath)].copy(),\n",
    "            data[data['athlete_hash'].isin(test_ath)].copy())\n",
    "\n",
    "train_df, val_df, test_df = random_athlete_split(df)\n",
    "\n",
    "train_ds = TriathlonDataset(train_df)\n",
    "val_ds = TriathlonDataset(val_df)\n",
    "test_ds = TriathlonDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=512, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_ds):,} | Val: {len(val_ds):,} | Test: {len(test_ds):,}\")\n",
    "for d in MODEL_DISTANCES:\n",
    "    n_tr = (train_df['event_distance'] == d).sum()\n",
    "    n_te = (test_df['event_distance'] == d).sum()\n",
    "    print(f\"  {d}: train={n_tr:,}  test={n_te:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f62c4",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "Categorical embeddings + continuous → Dense(128) → Dense(64) → Embedding(32) → 3 task heads.\n",
    "Distance embedding allows the model to learn distance-specific patterns internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f665ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 17,493\n",
      "AthleteEmbeddingNet(\n",
      "  (emb_gender): Embedding(2, 4)\n",
      "  (emb_age): Embedding(162, 8)\n",
      "  (emb_country): Embedding(51, 16)\n",
      "  (emb_dist): Embedding(2, 4)\n",
      "  (backbone): Sequential(\n",
      "    (0): Linear(in_features=37, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (head_time): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (head_splits): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (head_risk): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AthleteEmbeddingNet(nn.Module):\n",
    "    def __init__(self, n_gender, n_age, n_country, n_dist, n_cont,\n",
    "                 emb_dim=32):\n",
    "        super().__init__()\n",
    "        # Categorical embeddings\n",
    "        self.emb_gender = nn.Embedding(n_gender, 4)\n",
    "        self.emb_age = nn.Embedding(n_age, 8)\n",
    "        self.emb_country = nn.Embedding(n_country, 16)\n",
    "        self.emb_dist = nn.Embedding(n_dist, 4)\n",
    "\n",
    "        # Input dim = sum of embeddings + continuous\n",
    "        input_dim = 4 + 8 + 16 + 4 + n_cont  # 32 + n_cont\n",
    "\n",
    "        # Shared backbone\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Task heads\n",
    "        self.head_time = nn.Linear(emb_dim, 1)          # total time regression\n",
    "        self.head_splits = nn.Linear(emb_dim, 3)        # split ratios (softmax)\n",
    "        self.head_risk = nn.Linear(emb_dim, 1)          # DNF risk (sigmoid)\n",
    "\n",
    "    def forward(self, gender, age, country, distance, continuous):\n",
    "        g = self.emb_gender(gender)\n",
    "        a = self.emb_age(age)\n",
    "        c = self.emb_country(country)\n",
    "        d = self.emb_dist(distance)\n",
    "\n",
    "        x = torch.cat([g, a, c, d, continuous], dim=-1)\n",
    "        embedding = self.backbone(x)\n",
    "\n",
    "        time_pred = self.head_time(embedding).squeeze(-1)\n",
    "        split_pred = torch.softmax(self.head_splits(embedding), dim=-1)\n",
    "        risk_pred = torch.sigmoid(self.head_risk(embedding)).squeeze(-1)\n",
    "\n",
    "        return time_pred, split_pred, risk_pred, embedding\n",
    "\n",
    "model = AthleteEmbeddingNet(N_GENDER, N_AGE, N_COUNTRY, N_DIST, len(cont_features))\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3735c",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf37c1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "  Epoch  1: train_loss=0.0991  val_loss=0.0166  val_MAE=15.1min  ✓ best\n",
      "  Epoch  2: train_loss=0.0065  val_loss=0.0126  val_MAE=12.1min  ✓ best\n",
      "  Epoch  3: train_loss=0.0006  val_loss=0.0126  val_MAE=12.9min  ✓ best\n",
      "  Epoch  5: train_loss=-0.0015  val_loss=0.0138  val_MAE=14.4min  \n",
      "  Epoch  6: train_loss=-0.0018  val_loss=0.0125  val_MAE=13.7min  ✓ best\n",
      "  Epoch 10: train_loss=-0.0024  val_loss=0.0147  val_MAE=15.7min  \n",
      "  Early stopping at epoch 11\n",
      "\n",
      "Best model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Loss weights\n",
    "LAMBDA_TIME = 1.0\n",
    "LAMBDA_SPLITS = 0.5\n",
    "LAMBDA_RISK = 0.3\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3,\n",
    "                                           steps_per_epoch=len(train_loader),\n",
    "                                           epochs=30)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        time_pred, split_pred, risk_pred, _ = model(\n",
    "            batch['gender'], batch['age'], batch['country'],\n",
    "            batch['distance'], batch['continuous'])\n",
    "\n",
    "        loss_time = mse_loss(time_pred, batch['total'])\n",
    "        log_pred = torch.log(split_pred + 1e-8)\n",
    "        loss_splits = torch.nn.functional.kl_div(log_pred, batch['splits'],\n",
    "                                                  reduction='batchmean')\n",
    "        loss_risk = bce_loss(risk_pred, batch['dnf'])\n",
    "\n",
    "        loss = LAMBDA_TIME * loss_time + LAMBDA_SPLITS * loss_splits + LAMBDA_RISK * loss_risk\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * len(batch['gender'])\n",
    "        n += len(batch['gender'])\n",
    "    return total_loss / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "    for batch in loader:\n",
    "        time_pred, split_pred, risk_pred, _ = model(\n",
    "            batch['gender'], batch['age'], batch['country'],\n",
    "            batch['distance'], batch['continuous'])\n",
    "        loss_time = mse_loss(time_pred, batch['total'])\n",
    "        total_loss += loss_time.item() * len(batch['gender'])\n",
    "        n += len(batch['gender'])\n",
    "        all_preds.append(time_pred.numpy())\n",
    "        all_true.append(batch['total'].numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_true = np.concatenate(all_true)\n",
    "    preds_sec = all_preds * total_std + total_mean\n",
    "    true_sec = all_true * total_std + total_mean\n",
    "    mae = np.abs(preds_sec - true_sec).mean()\n",
    "    return total_loss / n, mae\n",
    "\n",
    "print(\"Training...\")\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(30):\n",
    "    t0 = time()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    val_loss, val_mae = eval_epoch(model, val_loader)\n",
    "\n",
    "    improved = val_loss < best_val_loss\n",
    "    if improved:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), str(CLEANED / 'embedding_model.pt'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or improved:\n",
    "        print(f\"  Epoch {epoch+1:2d}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n",
    "              f\"val_MAE={val_mae/60:.1f}min  {'✓ best' if improved else ''}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(str(CLEANED / 'embedding_model.pt'), weights_only=True))\n",
    "print(\"\\nBest model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c688df",
   "metadata": {},
   "source": [
    "## 5. Evaluate on Test Set (Per Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0a63b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OVERALL Test — MAE: 13.6 min | R²: 0.9879\n",
      "  70.3 Test — n=203,608  MAE: 10.4 min  R²: 0.8876\n",
      "  140.6 Test — n=231,132  MAE: 16.5 min  R²: 0.9385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Collect all test predictions\n",
    "model.eval()\n",
    "all_preds, all_true, all_embeddings, all_dists = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        time_pred, _, _, emb = model(\n",
    "            batch['gender'], batch['age'], batch['country'],\n",
    "            batch['distance'], batch['continuous'])\n",
    "        all_preds.append(time_pred.numpy())\n",
    "        all_true.append(batch['total'].numpy())\n",
    "        all_embeddings.append(emb.numpy())\n",
    "        all_dists.append(batch['distance'].numpy())\n",
    "\n",
    "preds_sec = np.concatenate(all_preds) * total_std + total_mean\n",
    "true_sec = np.concatenate(all_true) * total_std + total_mean\n",
    "all_dists_arr = np.concatenate(all_dists)\n",
    "\n",
    "# Overall\n",
    "mae = mean_absolute_error(true_sec, preds_sec)\n",
    "r2 = r2_score(true_sec, preds_sec)\n",
    "print(f\"OVERALL Test — MAE: {mae/60:.1f} min | R²: {r2:.4f}\")\n",
    "\n",
    "# Per-distance evaluation\n",
    "for d_idx, d_name in enumerate(MODEL_DISTANCES):\n",
    "    mask = all_dists_arr == d_idx\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    mae_d = mean_absolute_error(true_sec[mask], preds_sec[mask])\n",
    "    r2_d = r2_score(true_sec[mask], preds_sec[mask])\n",
    "    print(f\"  {d_name} Test — n={mask.sum():,}  MAE: {mae_d/60:.1f} min  R²: {r2_d:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c7529",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings for All Athletes\n",
    "\n",
    "Process full dataset in batches to extract 32-dim embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5455b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for full dataset...\n",
      "Embedding matrix: (3738813, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating embeddings for full dataset...\")\n",
    "full_ds = TriathlonDataset(df)\n",
    "full_loader = DataLoader(full_ds, batch_size=1024, shuffle=False, num_workers=0)\n",
    "\n",
    "all_embs = []\n",
    "all_time_preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        time_pred, _, _, emb = model(\n",
    "            batch['gender'], batch['age'], batch['country'],\n",
    "            batch['distance'], batch['continuous'])\n",
    "        all_embs.append(emb.numpy())\n",
    "        all_time_preds.append(time_pred.numpy())\n",
    "\n",
    "embeddings = np.concatenate(all_embs, axis=0)\n",
    "neural_preds_all = np.concatenate(all_time_preds) * total_std + total_mean\n",
    "print(f\"Embedding matrix: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c29313",
   "metadata": {},
   "source": [
    "## 7. \"Athletes Like You\" — Cross-Distance Similarity\n",
    "\n",
    "Find similar athletes via cosine similarity in embedding space.\n",
    "This works across distances — a 70.3 athlete can find comparable 140.6 athletes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d9b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query (70.3): nan | M 50-54 | total=5.86h\n",
      "  Top 5 similar athletes:\n",
      "    1. sim=1.000 | nan | M 40-44 | total=5.94h | dist=70.3\n",
      "    2. sim=1.000 | nan | M 40-44 | total=5.97h | dist=70.3\n",
      "    3. sim=1.000 | nan | M 45-49 | total=5.96h | dist=70.3\n",
      "    4. sim=1.000 | nan | M 35-39 | total=5.98h | dist=70.3\n",
      "    5. sim=1.000 | nan | M 35-39 | total=5.97h | dist=70.3\n",
      "\n",
      "Query (140.6): Peter Šimún | M nan | total=14.00h\n",
      "  Top 5 similar athletes:\n",
      "    1. sim=0.670 | nan | M 00 | total=4.18h | dist=70.3\n",
      "    2. sim=0.670 | nan | M 00 | total=3.89h | dist=70.3\n",
      "    3. sim=0.669 | nan | M 00 | total=3.88h | dist=70.3\n",
      "    4. sim=0.669 | nan | M 00 | total=3.90h | dist=70.3\n",
      "    5. sim=0.667 | nan | M 00 | total=4.25h | dist=70.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Demo: pick one athlete per distance\n",
    "for d in MODEL_DISTANCES:\n",
    "    mask = df['event_distance'].values == d\n",
    "    indices = np.where(mask)[0]\n",
    "    if len(indices) < 100:\n",
    "        continue\n",
    "    demo_idx = indices[42]\n",
    "    query_emb = embeddings[demo_idx:demo_idx+1]\n",
    "\n",
    "    # Compare against first 10K records\n",
    "    sims = cosine_similarity(query_emb, embeddings[:10000])[0]\n",
    "    top5_idx = np.argsort(sims)[-6:-1][::-1]\n",
    "\n",
    "    query_row = df.iloc[demo_idx]\n",
    "    print(f\"\\nQuery ({d}): {query_row.get('athlete_name', 'N/A')} | \"\n",
    "          f\"{query_row['gender']} {query_row['age_group']} | \"\n",
    "          f\"total={query_row['total_sec']/3600:.2f}h\")\n",
    "    print(f\"  Top 5 similar athletes:\")\n",
    "    for rank, idx in enumerate(top5_idx, 1):\n",
    "        row = df.iloc[idx]\n",
    "        sim = sims[idx]\n",
    "        print(f\"    {rank}. sim={sim:.3f} | {row.get('athlete_name', 'N/A')} | \"\n",
    "              f\"{row['gender']} {row['age_group']} | \"\n",
    "              f\"total={row['total_sec']/3600:.2f}h | dist={row['event_distance']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614efc5d",
   "metadata": {},
   "source": [
    "## 8. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0beb260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athlete_embeddings.csv: 3,738,813 rows × 32 dims\n",
      "neural_predictions_70.3.csv: 2,197,121\n",
      "neural_predictions_140.6.csv: 1,541,692\n",
      "neural_predictions.csv: 3,738,813\n",
      "\n",
      "✅ NEURAL EMBEDDINGS COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Embeddings CSV (full dataset)\n",
    "emb_cols = [f'emb_{i:02d}' for i in range(embeddings.shape[1])]\n",
    "emb_df = pd.DataFrame(embeddings, columns=emb_cols)\n",
    "emb_df.insert(0, 'athlete_hash', df['athlete_hash'].values)\n",
    "emb_df.insert(1, 'event_distance', df['event_distance'].values)\n",
    "emb_df.to_csv(CLEANED / 'athlete_embeddings.csv', index=False)\n",
    "print(f\"athlete_embeddings.csv: {len(emb_df):,} rows × {embeddings.shape[1]} dims\")\n",
    "\n",
    "# Neural predictions — save per-distance for consistency with other notebooks\n",
    "for d in MODEL_DISTANCES:\n",
    "    mask = df['event_distance'].values == d\n",
    "    neural_dist = pd.DataFrame({\n",
    "        'athlete_hash': df.loc[mask, 'athlete_hash'].values if isinstance(mask, pd.Series) else df['athlete_hash'].values[mask],\n",
    "        'event_distance': d,\n",
    "        'event_year': df['event_year'].values[mask],\n",
    "        'total_sec': df['total_sec'].values[mask],\n",
    "        'neural_pred': neural_preds_all[mask],\n",
    "    })\n",
    "    fname = f'neural_predictions_{d}.csv'\n",
    "    neural_dist.to_csv(CLEANED / fname, index=False)\n",
    "    print(f\"{fname}: {len(neural_dist):,}\")\n",
    "\n",
    "# Also save combined for backward compatibility\n",
    "neural_all = pd.DataFrame({\n",
    "    'athlete_hash': df['athlete_hash'].values,\n",
    "    'event_distance': df['event_distance'].values,\n",
    "    'total_sec': df['total_sec'].values,\n",
    "    'neural_pred': neural_preds_all,\n",
    "})\n",
    "neural_all.to_csv(CLEANED / 'neural_predictions.csv', index=False)\n",
    "print(f\"neural_predictions.csv: {len(neural_all):,}\")\n",
    "\n",
    "print(\"\\n✅ NEURAL EMBEDDINGS COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
