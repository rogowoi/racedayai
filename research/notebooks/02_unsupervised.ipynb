{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c36990",
   "metadata": {},
   "source": [
    "# Notebook 02 — Unsupervised Learning\n",
    "**RaceDayAI ML Prediction Engine (Plan 07)**\n",
    "\n",
    "Athlete clustering, pacing archetypes, UMAP visualization, anomaly detection.\n",
    "\n",
    "**Reads:** `athlete_race.csv`, `athlete_profile.csv`\n",
    "**Writes:** `cluster_assignments.csv`, `pacing_archetypes.csv`, `anomaly_flags.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb93667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dir: /Users/mykyta/projects/indie/racedayai/research/data/cleaned\n",
      "Files: ['athlete_race.csv', 'athlete_profile.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc, warnings\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE = Path('.').resolve().parent\n",
    "CLEANED = BASE / 'data' / 'cleaned'\n",
    "print(f\"Data dir: {CLEANED}\")\n",
    "print(f\"Files: {[f.name for f in CLEANED.glob('*.csv')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7690f8",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd70357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiles: 1,629,366 | Races: 4,124,345\n",
      "Profile columns: ['athlete_hash', 'athlete_name', 'gender', 'country', 'total_races', 'years_active', 'distances_raced', 'pb_swim_sec', 'pb_bike_sec', 'pb_run_sec', 'pb_total_sec', 'first_race_year', 'latest_race_year', 'improvement_slope', 'consistency_cv', 'swim_strength_z', 'bike_strength_z', 'run_strength_z', 'dominant_discipline', 'dnf_count', 'dnf_rate', 'avg_fade_ratio']\n"
     ]
    }
   ],
   "source": [
    "profiles = pd.read_csv(CLEANED / 'athlete_profile.csv', low_memory=False)\n",
    "races = pd.read_csv(CLEANED / 'athlete_race.csv',\n",
    "                    usecols=['athlete_hash','gender','age_group','event_distance',\n",
    "                             'swim_pct','bike_pct','run_pct','fade_ratio',\n",
    "                             'bike_run_ratio','is_pro','total_sec',\n",
    "                             'swim_sec','bike_sec','run_sec','t1_sec','t2_sec'],\n",
    "                    low_memory=False)\n",
    "print(f\"Profiles: {len(profiles):,} | Races: {len(races):,}\")\n",
    "print(f\"Profile columns: {list(profiles.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e848f8",
   "metadata": {},
   "source": [
    "## 2. Athlete Clustering\n",
    "\n",
    "Features: swim/bike/run strength z-scores, consistency, experience, improvement slope, fade ratio.\n",
    "Compare K-Means (silhouette sweep), GMM (BIC), and HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f5ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusterable athletes (3+ races, sufficient features): 301,730\n",
      "Feature matrix: (301730, 7)\n",
      "Feature means (post-scale): [-0.  0.  0.  0. -0.  0. -0.]\n"
     ]
    }
   ],
   "source": [
    "CLUSTER_FEATURES = [\n",
    "    'swim_strength_z', 'bike_strength_z', 'run_strength_z',\n",
    "    'consistency_cv', 'total_races', 'improvement_slope', 'avg_fade_ratio',\n",
    "]\n",
    "\n",
    "# Filter to athletes with 3+ races and sufficient features\n",
    "df = profiles[profiles['total_races'] >= 3].copy()\n",
    "valid = df[CLUSTER_FEATURES].notna().sum(axis=1) >= 5\n",
    "df = df[valid].copy()\n",
    "print(f\"Clusterable athletes (3+ races, sufficient features): {len(df):,}\")\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = df[CLUSTER_FEATURES].copy()\n",
    "X = X.fillna(X.median())\n",
    "for col in X.columns:\n",
    "    lo, hi = X[col].quantile([0.01, 0.99])\n",
    "    X[col] = X[col].clip(lo, hi)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(f\"Feature matrix: {X_scaled.shape}\")\n",
    "print(f\"Feature means (post-scale): {X_scaled.mean(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c4fb3a",
   "metadata": {},
   "source": [
    "### 2.1 K-Means Silhouette Sweep (k=5..20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  k= 5: sil=0.2382  ch=87826  db=1.316\n",
      "  k= 6: sil=0.2517  ch=84927  db=1.326\n",
      "  k= 7: sil=0.2608  ch=80001  db=1.314\n",
      "  k= 8: sil=0.2660  ch=77542  db=1.302\n",
      "  k= 9: sil=0.2188  ch=75044  db=1.337\n",
      "  k=10: sil=0.2227  ch=72703  db=1.292\n",
      "  k=11: sil=0.2280  ch=70672  db=1.262\n",
      "  k=12: sil=0.2256  ch=68211  db=1.306\n",
      "  k=13: sil=0.2319  ch=65981  db=1.249\n",
      "  k=14: sil=0.2173  ch=63629  db=1.302\n",
      "  k=15: sil=0.2286  ch=61619  db=1.296\n",
      "  k=16: sil=0.2055  ch=60120  db=1.342\n",
      "  k=17: sil=0.1949  ch=58626  db=1.341\n",
      "  k=18: sil=0.1945  ch=56820  db=1.347\n",
      "  k=19: sil=0.1951  ch=55562  db=1.344\n",
      "  k=20: sil=0.1867  ch=54043  db=1.369\n",
      "\n",
      "Best K-Means k=8 (silhouette=0.2660)\n"
     ]
    }
   ],
   "source": [
    "km_results = []\n",
    "for k in range(5, 21):\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=42, max_iter=300)\n",
    "    labels = km.fit_predict(X_scaled)\n",
    "    sil = silhouette_score(X_scaled, labels, sample_size=min(50000, len(X_scaled)))\n",
    "    ch = calinski_harabasz_score(X_scaled, labels)\n",
    "    db = davies_bouldin_score(X_scaled, labels)\n",
    "    km_results.append({'k': k, 'silhouette': sil, 'calinski_harabasz': ch, 'davies_bouldin': db})\n",
    "    print(f\"  k={k:2d}: sil={sil:.4f}  ch={ch:.0f}  db={db:.3f}\")\n",
    "\n",
    "km_results = pd.DataFrame(km_results)\n",
    "best_km_k = int(km_results.loc[km_results['silhouette'].idxmax(), 'k'])\n",
    "print(f\"\\nBest K-Means k={best_km_k} (silhouette={km_results['silhouette'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc7039",
   "metadata": {},
   "source": [
    "### 2.2 GMM with BIC Selection (k=5..15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22145e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  k= 5: BIC=3255435  AIC=3253535\n",
      "  k= 6: BIC=3194131  AIC=3191848\n",
      "  k= 7: BIC=2468153  AIC=2465488\n",
      "  k= 8: BIC=3077627  AIC=3074580\n",
      "  k= 9: BIC=2353592  AIC=2350162\n",
      "  k=10: BIC=2015323  AIC=2011511\n",
      "  k=11: BIC=2123025  AIC=2118831\n",
      "  k=12: BIC=1811825  AIC=1807249\n",
      "  k=13: BIC=1427618  AIC=1422660\n",
      "  k=14: BIC=1415540  AIC=1410200\n",
      "  k=15: BIC=1414765  AIC=1409043\n",
      "\n",
      "Best GMM k=15 (BIC=1414765)\n"
     ]
    }
   ],
   "source": [
    "gmm_results = []\n",
    "for k in range(5, 16):\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type='full',\n",
    "                          n_init=3, random_state=42, max_iter=200)\n",
    "    gmm.fit(X_scaled)\n",
    "    bic = gmm.bic(X_scaled)\n",
    "    aic = gmm.aic(X_scaled)\n",
    "    gmm_results.append({'k': k, 'bic': bic, 'aic': aic})\n",
    "    print(f\"  k={k:2d}: BIC={bic:.0f}  AIC={aic:.0f}\")\n",
    "\n",
    "gmm_results = pd.DataFrame(gmm_results)\n",
    "best_gmm_k = int(gmm_results.loc[gmm_results['bic'].idxmin(), 'k'])\n",
    "print(f\"\\nBest GMM k={best_gmm_k} (BIC={gmm_results['bic'].min():.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c375793",
   "metadata": {},
   "source": [
    "### 2.3 HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14af1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN: 2 clusters, 16,502 noise points (5.5%)\n",
      "Silhouette (excl. noise): 0.4346\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import hdbscan\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=500, min_samples=50,\n",
    "                                 metric='euclidean', cluster_selection_method='eom')\n",
    "    hdb_labels = clusterer.fit_predict(X_scaled)\n",
    "    n_clusters = len(set(hdb_labels)) - (1 if -1 in hdb_labels else 0)\n",
    "    n_noise = (hdb_labels == -1).sum()\n",
    "    print(f\"HDBSCAN: {n_clusters} clusters, {n_noise:,} noise points ({100*n_noise/len(hdb_labels):.1f}%)\")\n",
    "    if n_clusters >= 2:\n",
    "        valid_mask = hdb_labels != -1\n",
    "        sil = silhouette_score(X_scaled[valid_mask], hdb_labels[valid_mask],\n",
    "                               sample_size=min(50000, valid_mask.sum()))\n",
    "        print(f\"Silhouette (excl. noise): {sil:.4f}\")\n",
    "except ImportError:\n",
    "    print(\"HDBSCAN not installed, skipping\")\n",
    "    hdb_labels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544ce11",
   "metadata": {},
   "source": [
    "### 2.4 Final Clustering — K-Means with Best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f3241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using K-Means k=8\n",
      "\n",
      "Cluster Summary:\n",
      "  [0] Veteran                        n=15,962 (5.3%)\n",
      "       swim_z=0.30  bike_z=0.22  run_z=0.29  races=3.2  fade=-0.509\n",
      "  [1] StrongSwim_StrongBike_StrongRun n=37,216 (12.3%)\n",
      "       swim_z=1.06  bike_z=1.24  run_z=1.18  races=0.0  fade=-0.413\n",
      "  [2] WeakSwim_WeakBike_WeakRun      n=14,655 (4.9%)\n",
      "       swim_z=-1.26  bike_z=-1.49  run_z=-1.32  races=-0.4  fade=0.101\n",
      "  [3] WeakSwim_WeakBike_WeakRun      n=33,377 (11.1%)\n",
      "       swim_z=-0.51  bike_z=-0.80  run_z=-0.51  races=0.3  fade=-0.555\n",
      "  [4] StrongRun                      n=91,692 (30.4%)\n",
      "       swim_z=0.37  bike_z=0.38  run_z=0.52  races=-0.3  fade=-0.560\n",
      "  [5] WeakRun_Fader                  n=68,343 (22.7%)\n",
      "       swim_z=-0.32  bike_z=-0.21  run_z=-0.53  races=-0.3  fade=1.171\n",
      "  [6] WeakSwim_WeakBike_WeakRun      n=23,533 (7.8%)\n",
      "       swim_z=-1.55  bike_z=-1.77  run_z=-1.78  races=0.1  fade=0.639\n",
      "  [7] StrongSwim_StrongBike_StrongRun n=16,952 (5.6%)\n",
      "       swim_z=0.92  bike_z=1.18  run_z=1.08  races=-0.3  fade=-0.191\n"
     ]
    }
   ],
   "source": [
    "# Use best K-Means as primary (most interpretable)\n",
    "print(f\"Using K-Means k={best_km_k}\")\n",
    "km_final = KMeans(n_clusters=best_km_k, n_init=20, random_state=42)\n",
    "df['cluster_id'] = km_final.fit_predict(X_scaled)\n",
    "\n",
    "# GMM soft assignments\n",
    "gmm_final = GaussianMixture(n_components=best_gmm_k, covariance_type='full',\n",
    "                            n_init=5, random_state=42)\n",
    "gmm_final.fit(X_scaled)\n",
    "df['gmm_cluster'] = gmm_final.predict(X_scaled)\n",
    "gmm_probs = gmm_final.predict_proba(X_scaled)\n",
    "df['gmm_max_prob'] = gmm_probs.max(axis=1)\n",
    "\n",
    "if hdb_labels is not None:\n",
    "    df['hdbscan_cluster'] = hdb_labels\n",
    "\n",
    "# Cluster centroids\n",
    "centroids = pd.DataFrame(km_final.cluster_centers_, columns=CLUSTER_FEATURES)\n",
    "centroids['size'] = df['cluster_id'].value_counts().sort_index().values\n",
    "centroids['pct'] = 100 * centroids['size'] / centroids['size'].sum()\n",
    "\n",
    "# Auto-name clusters\n",
    "names = []\n",
    "for i, row in centroids.iterrows():\n",
    "    traits = []\n",
    "    if row['swim_strength_z'] > 0.5: traits.append('StrongSwim')\n",
    "    if row['bike_strength_z'] > 0.5: traits.append('StrongBike')\n",
    "    if row['run_strength_z'] > 0.5: traits.append('StrongRun')\n",
    "    if row['swim_strength_z'] < -0.5: traits.append('WeakSwim')\n",
    "    if row['bike_strength_z'] < -0.5: traits.append('WeakBike')\n",
    "    if row['run_strength_z'] < -0.5: traits.append('WeakRun')\n",
    "    if row['total_races'] > centroids['total_races'].median() + 1: traits.append('Veteran')\n",
    "    if row['total_races'] < centroids['total_races'].median() - 1: traits.append('Novice')\n",
    "    if row['avg_fade_ratio'] > 1.1: traits.append('Fader')\n",
    "    if row['improvement_slope'] < -100: traits.append('Improving')\n",
    "    name = '_'.join(traits[:3]) if traits else f'Cluster_{i}'\n",
    "    names.append(name)\n",
    "centroids['name'] = names\n",
    "df['cluster_name'] = df['cluster_id'].map(dict(enumerate(names)))\n",
    "\n",
    "print(\"\\nCluster Summary:\")\n",
    "for i, row in centroids.iterrows():\n",
    "    print(f\"  [{i}] {row['name']:30s} n={int(row['size']):,} ({row['pct']:.1f}%)\")\n",
    "    print(f\"       swim_z={row['swim_strength_z']:.2f}  bike_z={row['bike_strength_z']:.2f}  \"\n",
    "          f\"run_z={row['run_strength_z']:.2f}  races={row['total_races']:.1f}  fade={row['avg_fade_ratio']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d08ae5",
   "metadata": {},
   "source": [
    "### 2.5 UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd50a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP not installed, skipping visualization\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from umap import UMAP\n",
    "\n",
    "    n_umap = min(100000, len(X_scaled))\n",
    "    idx = np.random.RandomState(42).choice(len(X_scaled), n_umap, replace=False)\n",
    "    print(f\"Running UMAP on {n_umap:,} points...\")\n",
    "\n",
    "    reducer = UMAP(n_components=2, n_neighbors=30, min_dist=0.3, random_state=42)\n",
    "    emb_2d = reducer.fit_transform(X_scaled[idx])\n",
    "\n",
    "    umap_df = pd.DataFrame({\n",
    "        'athlete_hash': df.iloc[idx]['athlete_hash'].values,\n",
    "        'umap_x': emb_2d[:, 0], 'umap_y': emb_2d[:, 1],\n",
    "        'cluster_id': df.iloc[idx]['cluster_id'].values,\n",
    "    })\n",
    "    umap_df.to_csv(CLEANED / 'umap_coords.csv', index=False)\n",
    "    print(f\"Saved UMAP coordinates: {len(umap_df):,} points\")\n",
    "\n",
    "    # Quick scatter plot\n",
    "    try:\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        scatter = ax.scatter(emb_2d[:, 0], emb_2d[:, 1],\n",
    "                            c=df.iloc[idx]['cluster_id'].values,\n",
    "                            cmap='tab10', s=1, alpha=0.3)\n",
    "        ax.set_title('UMAP — Athlete Clusters')\n",
    "        ax.set_xlabel('UMAP-1'); ax.set_ylabel('UMAP-2')\n",
    "        plt.colorbar(scatter, label='Cluster ID')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(CLEANED / 'umap_clusters.png', dpi=150)\n",
    "        plt.show()\n",
    "        print(\"Saved umap_clusters.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Plotting failed: {e}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"UMAP not installed, skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b2378",
   "metadata": {},
   "source": [
    "## 3. Pacing Archetypes\n",
    "\n",
    "GMM on [swim_pct, bike_pct, run_pct, fade_ratio] per distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records with complete pacing data: 3,720,788\n",
      "\n",
      "[70.3] 2,085,244 records\n",
      "  Best GMM components: 7\n",
      "  [0] AggressiveBike                 n=122,374  swim=0.090 bike=0.557 run=0.330 fade=1.026 med_total=6.31h\n",
      "  [1] HeavyFade                      n=25,196  swim=0.152 bike=0.448 run=0.374 fade=1.258 med_total=6.83h\n",
      "  [2] HeavyFade_ConservativeBike     n=20,961  swim=0.100 bike=0.404 run=0.479 fade=1.553 med_total=6.92h\n",
      "  [3] HeavyFade_ConservativeBike     n=469,765  swim=0.105 bike=0.475 run=0.394 fade=1.312 med_total=6.80h\n",
      "  [4] StrongRun                      n=34,829  swim=0.157 bike=0.510 run=0.312 fade=0.877 med_total=5.80h\n",
      "  [5] AggressiveBike_StrongRun       n=453,447  swim=0.112 bike=0.534 run=0.333 fade=0.816 med_total=5.00h\n",
      "  [6] Balanced                       n=958,672  swim=0.113 bike=0.507 run=0.353 fade=0.994 med_total=5.77h\n",
      "\n",
      "[140.6] 1,408,771 records\n",
      "  Best GMM components: 7\n",
      "  [0] StrongRun                      n=122,485  swim=0.119 bike=0.509 run=0.358 fade=0.898 med_total=11.42h\n",
      "  [1] HeavyFade                      n=265,444  swim=0.094 bike=0.496 run=0.386 fade=1.159 med_total=13.78h\n",
      "  [2] HeavyFade_ConservativeBike     n=69,432  swim=0.097 bike=0.441 run=0.439 fade=1.347 med_total=14.25h\n",
      "  [3] HeavyFade_ConservativeBike     n=189,845  swim=0.108 bike=0.476 run=0.398 fade=1.178 med_total=13.44h\n",
      "  [4] Balanced                       n=429,709  swim=0.100 bike=0.508 run=0.374 fade=0.978 med_total=11.87h\n",
      "  [5] AggressiveBike_StrongRun       n=265,346  swim=0.107 bike=0.524 run=0.356 fade=0.809 med_total=10.35h\n",
      "  [6] Balanced                       n=66,510  swim=0.079 bike=0.519 run=0.383 fade=1.073 med_total=12.92h\n",
      "\n",
      "Pacing archetypes assigned: 3,494,015\n"
     ]
    }
   ],
   "source": [
    "pac_cols = ['swim_pct', 'bike_pct', 'run_pct', 'fade_ratio']\n",
    "pac_df = races.dropna(subset=pac_cols).copy()\n",
    "pac_df = pac_df[pac_df['is_pro'] != True]\n",
    "print(f\"Records with complete pacing data: {len(pac_df):,}\")\n",
    "\n",
    "results_by_dist = {}\n",
    "\n",
    "for dist in ['70.3', '140.6']:\n",
    "    subset = pac_df[pac_df['event_distance'] == dist]\n",
    "    if len(subset) < 1000:\n",
    "        print(f\"[{dist}] Too few records ({len(subset)}), skipping\")\n",
    "        continue\n",
    "    print(f\"\\n[{dist}] {len(subset):,} records\")\n",
    "\n",
    "    X_pac = subset[pac_cols].values\n",
    "    X_pac = np.clip(X_pac, np.percentile(X_pac, 1, axis=0), np.percentile(X_pac, 99, axis=0))\n",
    "    pac_scaler = StandardScaler()\n",
    "    X_pac_scaled = pac_scaler.fit_transform(X_pac)\n",
    "\n",
    "    # GMM with BIC\n",
    "    best_bic, best_k = np.inf, 4\n",
    "    for k in range(3, 9):\n",
    "        gmm = GaussianMixture(n_components=k, covariance_type='full',\n",
    "                              n_init=3, random_state=42, max_iter=200)\n",
    "        gmm.fit(X_pac_scaled)\n",
    "        bic = gmm.bic(X_pac_scaled)\n",
    "        if bic < best_bic:\n",
    "            best_bic, best_k = bic, k\n",
    "    print(f\"  Best GMM components: {best_k}\")\n",
    "\n",
    "    gmm = GaussianMixture(n_components=best_k, covariance_type='full',\n",
    "                          n_init=5, random_state=42)\n",
    "    gmm.fit(X_pac_scaled)\n",
    "    labels = gmm.predict(X_pac_scaled)\n",
    "    probs = gmm.predict_proba(X_pac_scaled)\n",
    "\n",
    "    for k_i in range(best_k):\n",
    "        mask = labels == k_i\n",
    "        means = subset.loc[mask, pac_cols].mean()\n",
    "        med_total = subset.loc[mask, 'total_sec'].median()\n",
    "        archetype = []\n",
    "        if means['bike_pct'] > subset['bike_pct'].median() + 0.02: archetype.append('AggressiveBike')\n",
    "        if means['fade_ratio'] > 1.08: archetype.append('HeavyFade')\n",
    "        if means['fade_ratio'] < 0.95: archetype.append('StrongRun')\n",
    "        if means['run_pct'] > subset['run_pct'].median() + 0.02: archetype.append('ConservativeBike')\n",
    "        if not archetype: archetype.append('Balanced')\n",
    "        name = '_'.join(archetype)\n",
    "        print(f\"  [{k_i}] {name:30s} n={mask.sum():,}  \"\n",
    "              f\"swim={means['swim_pct']:.3f} bike={means['bike_pct']:.3f} \"\n",
    "              f\"run={means['run_pct']:.3f} fade={means['fade_ratio']:.3f} \"\n",
    "              f\"med_total={med_total/3600:.2f}h\")\n",
    "\n",
    "    results_by_dist[dist] = (subset.index, labels, probs)\n",
    "\n",
    "# Build output\n",
    "all_labels = pd.Series(np.nan, index=races.index, dtype='float')\n",
    "all_probs = pd.Series(np.nan, index=races.index, dtype='float')\n",
    "for dist, (idx, labels, probs) in results_by_dist.items():\n",
    "    all_labels.loc[idx] = labels\n",
    "    all_probs.loc[idx] = probs.max(axis=1)\n",
    "\n",
    "print(f\"\\nPacing archetypes assigned: {all_labels.notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af84c7",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection\n",
    "\n",
    "Rule-based flags + Isolation Forest per distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based flags:\n",
      "  sum_mismatch: 941,572\n",
      "  bike_pct_extreme: 7\n",
      "  run_pct_extreme: 113\n",
      "\n",
      "Isolation Forest per distance...\n",
      "  [70.3] 21,436 anomalies (1.0%)\n",
      "  [140.6] 15,026 anomalies (1.0%)\n",
      "  [olympic] 2,380 anomalies (1.0%)\n",
      "  [sprint] 1,034 anomalies (1.0%)\n",
      "  [100km] 5 anomalies (1.1%)\n",
      "\n",
      "Total anomalies: 971,055 (23.54%)\n"
     ]
    }
   ],
   "source": [
    "flags = pd.DataFrame(index=races.index)\n",
    "\n",
    "# Sum check\n",
    "computed_sum = (races['swim_sec'].fillna(0) + races['bike_sec'].fillna(0) +\n",
    "                races['run_sec'].fillna(0) + races['t1_sec'].fillna(0) + races['t2_sec'].fillna(0))\n",
    "has_all = races['swim_sec'].notna() & races['bike_sec'].notna() & races['run_sec'].notna()\n",
    "flags['sum_mismatch'] = has_all & ((races['total_sec'] - computed_sum).abs() > 120)\n",
    "\n",
    "# Extreme transitions\n",
    "flags['extreme_t1'] = races['t1_sec'].notna() & (races['t1_sec'] > 900)\n",
    "flags['extreme_t2'] = races['t2_sec'].notna() & (races['t2_sec'] > 900)\n",
    "\n",
    "# Impossible split ratios\n",
    "for seg in ['swim', 'bike', 'run']:\n",
    "    col = f'{seg}_pct'\n",
    "    if col in races.columns:\n",
    "        flags[f'{seg}_pct_extreme'] = races[col].notna() & (\n",
    "            (races[col] < 0.02) | (races[col] > 0.70))\n",
    "\n",
    "print(\"Rule-based flags:\")\n",
    "for col in flags.columns:\n",
    "    n = flags[col].sum()\n",
    "    if n > 0:\n",
    "        print(f\"  {col}: {n:,}\")\n",
    "\n",
    "# Isolation Forest per distance\n",
    "print(\"\\nIsolation Forest per distance...\")\n",
    "iso_cols = ['swim_sec', 'bike_sec', 'run_sec', 'total_sec']\n",
    "flags['isolation_forest'] = False\n",
    "\n",
    "for dist in races['event_distance'].dropna().unique():\n",
    "    subset = races[races['event_distance'] == dist].dropna(subset=iso_cols)\n",
    "    if len(subset) < 100:\n",
    "        continue\n",
    "    X_iso = subset[iso_cols].values\n",
    "    iso = IsolationForest(contamination=0.01, random_state=42, n_jobs=-1)\n",
    "    preds = iso.fit_predict(X_iso)\n",
    "    anomalies = preds == -1\n",
    "    flags.loc[subset.index[anomalies], 'isolation_forest'] = True\n",
    "    print(f\"  [{dist}] {anomalies.sum():,} anomalies ({100*anomalies.mean():.1f}%)\")\n",
    "\n",
    "# Combined\n",
    "flags['is_anomaly'] = flags.any(axis=1)\n",
    "flags['reason'] = flags.apply(\n",
    "    lambda r: ','.join([c for c in flags.columns if c not in ('is_anomaly','reason') and r[c]]),\n",
    "    axis=1)\n",
    "flags['reason'] = flags['reason'].replace('', np.nan)\n",
    "\n",
    "n_anom = flags['is_anomaly'].sum()\n",
    "print(f\"\\nTotal anomalies: {n_anom:,} ({100*n_anom/len(races):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e046dc",
   "metadata": {},
   "source": [
    "## 5. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_assignments.csv: 301,730\n",
      "pacing_archetypes.csv: 3,494,015 assigned\n",
      "anomaly_flags.csv: 971,055 anomalies\n",
      "\n",
      "✅ UNSUPERVISED COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Cluster assignments\n",
    "cluster_out = df[['athlete_hash', 'cluster_id', 'cluster_name',\n",
    "                   'gmm_cluster', 'gmm_max_prob']].copy()\n",
    "if 'hdbscan_cluster' in df.columns:\n",
    "    cluster_out['hdbscan_cluster'] = df['hdbscan_cluster']\n",
    "cluster_out.to_csv(CLEANED / 'cluster_assignments.csv', index=False)\n",
    "centroids.to_csv(CLEANED / 'cluster_centroids.csv', index=False)\n",
    "print(f\"cluster_assignments.csv: {len(cluster_out):,}\")\n",
    "\n",
    "# Pacing archetypes\n",
    "pac_out = pd.DataFrame({\n",
    "    'pacing_archetype': all_labels,\n",
    "    'pacing_confidence': all_probs,\n",
    "})\n",
    "pac_out.to_csv(CLEANED / 'pacing_archetypes.csv', index=False)\n",
    "print(f\"pacing_archetypes.csv: {pac_out['pacing_archetype'].notna().sum():,} assigned\")\n",
    "\n",
    "# Anomaly flags\n",
    "anomaly_out = flags[['is_anomaly', 'reason']]\n",
    "anomaly_out.to_csv(CLEANED / 'anomaly_flags.csv', index=False)\n",
    "print(f\"anomaly_flags.csv: {flags['is_anomaly'].sum():,} anomalies\")\n",
    "\n",
    "print(\"\\n✅ UNSUPERVISED COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef734518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
